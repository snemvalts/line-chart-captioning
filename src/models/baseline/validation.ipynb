{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5. Putting it all together","provenance":[{"file_id":"1Sz28emk69WDFXdYKizkanXYsQWqtmKxU","timestamp":1612546786476}],"collapsed_sections":[],"authorship_tag":"ABX9TyM2oaxyyfCFAzTioQVXIeta"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e4c4b8d3415846339cd62bce8bc27780":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7caeb0b4f2284cbfb46b2e0a56f55ea4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b3b7c2267c6649eaa98f44592e0cd91b","IPY_MODEL_303e5bf509b341bc952665c83df7cc8f"]}},"7caeb0b4f2284cbfb46b2e0a56f55ea4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b3b7c2267c6649eaa98f44592e0cd91b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f16816270475429294e3840c735d175e","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_63ea8cc7333b4140903ede82e7e1f1dd"}},"303e5bf509b341bc952665c83df7cc8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5186dbaf9178496883723d5fbfc2f029","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [01:06&lt;00:00, 1.54MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d7169966c31b4927949621bb450ba78e"}},"f16816270475429294e3840c735d175e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"63ea8cc7333b4140903ede82e7e1f1dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5186dbaf9178496883723d5fbfc2f029":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d7169966c31b4927949621bb450ba78e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"NnhyS1VLNrfh"},"source":["# Starting out"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SAoN8e62MAif","executionInfo":{"status":"ok","timestamp":1619810753562,"user_tz":-180,"elapsed":27887,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}},"outputId":"13d19488-5cd8-4f6c-f7b2-f7213a71ad4b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wncr2153ROgV","executionInfo":{"status":"ok","timestamp":1619811112109,"user_tz":-180,"elapsed":386430,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["!cp /content/drive/MyDrive/Kool/Ülikool/'3. aasta'/Lõputöö/data.zip .\n","!unzip -q data.zip\n","!rm data.zip\n","\n","!cp /content/drive/MyDrive/Kool/Ülikool/'3. aasta'/Lõputöö/test_val.zip .\n","!unzip -q test_val.zip\n","!rm test_val.zip"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"2RgggMjkYEDc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619811118898,"user_tz":-180,"elapsed":393213,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}},"outputId":"d27bd166-46b6-4c14-dd82-3ea83c3c2f6d"},"source":["!rm -rf line-chart-captioning/\n","#clone repo\n","!git clone https://github.com/snemvalts/line-chart-captioning\n","#clean out data folder and move extracted raw data to captioning\n","!rm -rf line-chart-captioning/data/*\n","!mv data/** line-chart-captioning/data/\n","!mv test_val/validation1 line-chart-captioning/data/figureqa/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'line-chart-captioning'...\n","remote: Enumerating objects: 157, done.\u001b[K\n","remote: Counting objects: 100% (157/157), done.\u001b[K\n","remote: Compressing objects: 100% (114/114), done.\u001b[K\n","remote: Total 157 (delta 50), reused 122 (delta 25), pack-reused 0\u001b[K\n","Receiving objects: 100% (157/157), 232.27 KiB | 2.13 MiB/s, done.\n","Resolving deltas: 100% (50/50), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FjFeVDn2_pEo","executionInfo":{"status":"ok","timestamp":1619811118899,"user_tz":-180,"elapsed":393212,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["import json\n","\n","# Smoothest and roughest seem to always appear. Same for MIN_AUC and MAX_AUC\n","question_types = [\"GREATER\", \"LESS\", \"INTERSECT\"]\n","synthetic_json = {\n","  \"questions\": question_types\n","}\n","\n","with open('line-chart-captioning/synthetic.json', 'w') as f:\n","  json.dump(synthetic_json, f)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRinJJYLBCbk","executionInfo":{"status":"ok","timestamp":1619811119396,"user_tz":-180,"elapsed":393703,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}},"outputId":"6fdc657a-9b97-41e1-8e92-49baeb126a35"},"source":["!ls line-chart-captioning/data/figureqa"],"execution_count":5,"outputs":[{"output_type":"stream","text":["README.md  sample_train1  train1  validation1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUouFsnJ1sZi","executionInfo":{"status":"ok","timestamp":1619811236440,"user_tz":-180,"elapsed":510741,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}},"outputId":"909d2c59-02bf-4b5f-d8ed-0f37582a62ef"},"source":["!cd line-chart-captioning && \\\n","python3 src/synthetic/preprocess-question-types.py \\\n","--synthetic-config synthetic.json \\\n","data/figureqa/train1"],"execution_count":6,"outputs":[{"output_type":"stream","text":["parsing QA...\n","parsing annotations...\n","processing plots...\n","copying images...\n","writing csv...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qxftKh1i7rNf","executionInfo":{"status":"ok","timestamp":1619811256721,"user_tz":-180,"elapsed":531016,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}},"outputId":"58e19540-a251-46fe-91b4-86221ee44e96"},"source":["!cd line-chart-captioning && \\\n","python3 src/synthetic/preprocess-question-types.py \\\n","--synthetic-config synthetic.json \\\n","data/figureqa/validation1"],"execution_count":7,"outputs":[{"output_type":"stream","text":["parsing QA...\n","parsing annotations...\n","processing plots...\n","copying images...\n","writing csv...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Eg4SToRdybsI"},"source":["## creating transforms & dataset"]},{"cell_type":"code","metadata":{"id":"wQu5ntlOoscT","executionInfo":{"status":"ok","timestamp":1619811262188,"user_tz":-180,"elapsed":536481,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from PIL import Image\n","from skimage import transform\n","\n","\n","# adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n","class SyntheticImageDataset(Dataset):\n","  def __init__(self, csv_file, images_dir, transform=None):\n","    self.charts_captions = pd.read_csv(csv_file)\n","    self.images_dir = images_dir\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.charts_captions)\n","\n","  def __getitem__(self, idx):\n","    [image_number, description_blob, all_subjects_blob] = self.charts_captions.iloc[idx]\n","    image_path = os.path.join(self.images_dir, f'{str(image_number)}.png')\n","    image = np.array(Image.open(image_path))\n","    \n","    sample = {\n","        'image': image, \n","        'original_image': image,\n","        'description_blob': description_blob,\n","        'all_subjects_blob': all_subjects_blob   \n","    }\n","\n","    if self.transform:\n","      sample = self.transform(sample)\n","\n","    return sample"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"-o2x0Jb8MDdM","executionInfo":{"status":"ok","timestamp":1619811262191,"user_tz":-180,"elapsed":536482,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["class ResizeImage(object):\n","  def __init__(self, output_size):\n","    assert isinstance(output_size, (int, tuple))\n","    self.output_size = output_size\n","\n","  def __call__(self, sample):\n","    resized_image = transform.resize(sample['image'], self.output_size)\n","    return {**sample, 'image': resized_image}\n","\n","class StripImageTransparency(object):\n","  def __init__(self):\n","    pass\n","\n","  def __call__(self, sample):\n","    stripped_transparency_image = sample['image'][:,:,:3]\n","    return {**sample, 'image': stripped_transparency_image}\n","\n","\n","class NormalizeImage(object):\n","  def __init__(self, mean, std):\n","    self.mean = mean\n","    self.std = std\n","\n","  def __call__(self, sample):\n","    normalized_image = (sample['image'] - self.mean) / self.std\n","    return {**sample, 'image': normalized_image}\n","\n","\n","class ImageToTensor(object):\n","  def __init__(self):\n","    pass\n","\n","  def __call__(self, sample):\n","    image = sample['image'].transpose((2, 0, 1))\n","    return {**sample, 'image': torch.tensor(image).float()}\n","\n","\n","class FirstQuestionTypeToOneHotTensor(object):\n","  def __init__(self):\n","    pass\n","\n","  def __call__(self, sample):\n","    descriptions = json.loads(sample['description_blob'])\n","    first_description = descriptions[0]\n","    first_description_type = first_description[0]\n","\n","    all_description_types = [0]*(len(question_types))\n","\n","    description_type_index = question_types.index(first_description_type)\n","    all_description_types[description_type_index] = 1\n","\n","    return {**sample, 'first_description_type': torch.tensor(all_description_types).float() }\n","\n","\n","class IncludeSubjectLengths(object):\n","  def __init__(self):\n","    pass\n","\n","  def __call__(self, sample):\n","    all_subjects_len = len(json.loads(sample['all_subjects_blob']))\n","    return {**sample, 'subject_lengths': all_subjects_len }\n","\n","class FirstQuestionSubjectsToOneHotTensor(object):\n","  def __init__(self, max_len):\n","    self.max_len = max_len\n","    pass\n","\n","  def __call__(self, sample):\n","    descriptions = json.loads(sample['description_blob'])\n","    all_subjects = json.loads(sample['all_subjects_blob'])\n","\n","    first_description = descriptions[0]\n","    first_description_first_subject = first_description[1]\n","    first_description_second_subject = first_description[2]\n","\n","    first_subject_onehot = [0]*self.max_len\n","    first_description_subject_index = all_subjects.index(first_description_first_subject)\n","    first_subject_onehot[first_description_subject_index] = 1\n","\n","    second_subject_onehot = [0]*self.max_len\n","    second_description_subject_index = all_subjects.index(first_description_second_subject)\n","    second_subject_onehot[second_description_subject_index] = 1\n","\n","    return {\n","        **sample, \n","        'first_description_subject_onehot': torch.tensor(first_subject_onehot).long(),\n","        'first_description_subject_index': first_description_subject_index,\n","        'second_description_subject_onehot': torch.tensor(second_subject_onehot).long(),\n","        'second_description_subject_index': second_description_subject_index\n","    }\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFYGum8CMi4h","executionInfo":{"status":"ok","timestamp":1619811262192,"user_tz":-180,"elapsed":536482,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["import torch\n","\n","def get_dataset(images_dir = None, csv_file = None):\n","  # parallely calculate max caption len and word map to pass it to padcaption\n","  # cause can't access dataset from transforms before it is defined in compose\n","\n","  captions_csv = pd.read_csv(csv_file)\n","  \n","  max_subjects_len = captions_csv['all_subjects_blob'].apply(lambda blob: len(json.loads(blob))).max()\n","  \n","\n","  train_dataset = SyntheticImageDataset(images_dir = images_dir,\n","                           csv_file = csv_file,\n","                           transform=transforms.Compose([\n","                                  ResizeImage((224, 224)),\n","                                  StripImageTransparency(),\n","                                  NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","                                  ImageToTensor(),\n","                                  IncludeSubjectLengths()\n","                                 ]))\n","                                 \n","\n","  return train_dataset, max_subjects_len\n","\n","\n","train_dataset, max_subjects_len = get_dataset(images_dir = 'line-chart-captioning/data/processed_synthetic/train1-types/images',\n","                      csv_file = 'line-chart-captioning/data/processed_synthetic/train1-types/captions.csv')\n","\n","validation_dataset = SyntheticImageDataset(images_dir = 'line-chart-captioning/data/processed_synthetic/validation1-types/images',\n","                           csv_file = 'line-chart-captioning/data/processed_synthetic/validation1-types/captions.csv',\n","                           transform=transforms.Compose([\n","                                  ResizeImage((224, 224)),\n","                                  StripImageTransparency(),\n","                                  NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","                                  ImageToTensor(),\n","                                  IncludeSubjectLengths()\n","                                 ]))\n","\n","\n","\n","batch_size=1\n","\n","train_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","\n","val_loader = torch.utils.data.DataLoader(\n","        validation_dataset,\n","        batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"kpJfG3k43qfX","executionInfo":{"status":"ok","timestamp":1619811494587,"user_tz":-180,"elapsed":924,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}},"outputId":"9d848b3b-03f0-4b67-f696-0e30002f819a"},"source":["pd_train = pd.read_csv('line-chart-captioning/data/processed_synthetic/validation1-types/captions.csv')\n","#pd_train['all_subjects_blob']\n","print(pd_train.apply(lambda row: len(json.loads(row['description_blob'])), axis=1).sum())\n","pd_train.groupby('number').nunique()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["9125\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>description_blob</th>\n","      <th>all_subjects_blob</th>\n","    </tr>\n","    <tr>\n","      <th>number</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>19976</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19979</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19983</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19986</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19989</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3781 rows × 2 columns</p>\n","</div>"],"text/plain":["        description_blob  all_subjects_blob\n","number                                     \n","5                      1                  1\n","8                      1                  1\n","9                      1                  1\n","12                     1                  1\n","13                     1                  1\n","...                  ...                ...\n","19976                  1                  1\n","19979                  1                  1\n","19983                  1                  1\n","19986                  1                  1\n","19989                  1                  1\n","\n","[3781 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"4VNbi6_FykrD"},"source":["## encoder and elements model"]},{"cell_type":"code","metadata":{"id":"uCMmvlNbknM1","executionInfo":{"status":"ok","timestamp":1619811262680,"user_tz":-180,"elapsed":536963,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["import torchvision\n","from torch import nn\n","\n","class Encoder(nn.Module):\n","  def __init__(self, encoder_shape):\n","    super(Encoder, self).__init__()\n","\n","    base_resnet = torchvision.models.resnet50(pretrained=True) \n","    resnet_without_fc = nn.Sequential(*(list(base_resnet.children())[:-2]))\n","\n","    # freeze weights of resnet \n","    #for parameter in resnet_without_fc.parameters():\n","    #  parameter.requires_grad = False\n","\n","\n","    self.resnet = resnet_without_fc\n","    self.pool = nn.AdaptiveAvgPool3d(encoder_shape)\n","\n","  \n","  def forward(self, x):\n","    x = self.resnet(x)\n","    x = self.pool(x)\n","    x = torch.reshape(x, (-1, 2048*3*3))\n","    return x\n","\n","class SubjectsModel(nn.Module):\n","  def __init__(self, encoder_shape, \n","               category_count,\n","               subject_max_len, \n","               cut_off=True):\n","    super(SubjectsModel, self).__init__()\n","\n","    encoder_dim = 1\n","    for dim in encoder_shape:\n","      encoder_dim *= dim\n","\n","    self.question_type_dense = nn.Linear(category_count, 1024)\n","    self.other_subject_dense = nn.Linear(subject_max_len, 1024)\n","    self.encoder_dense = nn.Linear(encoder_dim, 1024)\n","\n","    hidden_dim = 1024 * 3\n","\n","    self.hidden1 = nn.Linear(hidden_dim, hidden_dim)\n","    self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n","    self.hidden3 = nn.Linear(hidden_dim, hidden_dim)\n","    self.hidden4 = nn.Linear(hidden_dim, hidden_dim)\n","    self.hidden5 = nn.Linear(hidden_dim, hidden_dim)\n","\n","    self.output = nn.Linear(hidden_dim, subject_max_len)\n","\n","    self.relu = nn.ReLU()\n","\n","    self.cut_off = cut_off\n","    self.softmax = nn.Softmax()\n","\n","  def forward(self, encoder_out, question_type_onehot, other_subject_onehot, other_subject_indices=[], subject_lengths=[]):\n","    batch_size = encoder_out.shape[0]\n","\n","    x_category = self.question_type_dense(question_type_onehot)\n","    x_subject = self.other_subject_dense(other_subject_onehot)\n","    x_encoder = self.encoder_dense(encoder_out)\n","\n","    x = torch.cat((x_category, x_subject, x_encoder), dim=1)\n","\n","    x = self.relu(self.hidden1(x))\n","    x = self.relu(self.hidden2(x))\n","    x = self.relu(self.hidden3(x))\n","\n","    x = self.relu(self.hidden4(x))\n","    x = self.relu(self.hidden5(x))\n","\n","    x = self.output(x)\n","    x = self.softmax(x)\n","\n","\n","    x_cloned = x.clone()\n","    # if cut off is enabled, use subject lengths to set any unnecessary predictions to 0\n","    if (self.cut_off):\n","        assert len(subject_lengths) == batch_size, 'Subject lengths not provided'\n","        for i, length in enumerate(subject_lengths):\n","          #print('before', x_cloned[i], 'with length ', length)\n","          x_cloned[i, length:] = 0.0\n","          \n","\n","        # reset the other subject prediction to 0 as well \n","        for i, other_subject_index in enumerate(other_subject_indices):\n","          x_cloned[i][other_subject_index] = 0.0\n","\n","    return x_cloned"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRl4FKM4_wyk"},"source":["# categories model"]},{"cell_type":"code","metadata":{"id":"0pWtxu-P_73U","executionInfo":{"status":"ok","timestamp":1619811262681,"user_tz":-180,"elapsed":536963,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["class CategoryModel(nn.Module):\n","  def __init__(self, encoder_shape, \n","               hidden_dim, \n","               category_count,\n","               dropout_p=0.5):\n","    super(CategoryModel, self).__init__()\n","\n","    encoder_dim = 1\n","    for dim in encoder_shape:\n","      encoder_dim *= dim\n","\n","\n","    self.hidden1 = nn.Linear(encoder_dim, hidden_dim)\n","    self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n","    self.hidden3 = nn.Linear(hidden_dim, hidden_dim)\n","    self.hidden4 = nn.Linear(hidden_dim, hidden_dim)\n","    self.hidden5 = nn.Linear(hidden_dim, hidden_dim)\n","    self.hidden6 = nn.Linear(hidden_dim, hidden_dim)\n","\n","\n","    self.output = nn.Linear(hidden_dim, category_count)\n","\n","    self.relu = nn.ReLU()\n","\n","    self.softmax = nn.Softmax()\n","\n","  def forward(self, encoder_out):\n","    x = self.relu(self.hidden1(encoder_out))\n","    x = self.relu(self.hidden2(x))\n","    x = self.relu(self.hidden3(x))\n","\n","    x = self.relu(self.hidden4(x))\n","    x = self.relu(self.hidden5(x))\n","    x = self.relu(self.hidden6(x))\n","\n","    x = self.output(x)\n","    x = self.softmax(x)\n","    #predicted_types = torch.argmax(x, dim=1)\n","    #print(predicted_types)\n","    #print()\n","    return x #predictions if we want the whole arr\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERC8muQgCErD"},"source":["# loading both models\n"]},{"cell_type":"code","metadata":{"id":"YTW2AAV0CEDu","colab":{"base_uri":"https://localhost:8080/","height":458,"referenced_widgets":["e4c4b8d3415846339cd62bce8bc27780","7caeb0b4f2284cbfb46b2e0a56f55ea4","b3b7c2267c6649eaa98f44592e0cd91b","303e5bf509b341bc952665c83df7cc8f","f16816270475429294e3840c735d175e","63ea8cc7333b4140903ede82e7e1f1dd","5186dbaf9178496883723d5fbfc2f029","d7169966c31b4927949621bb450ba78e"]},"executionInfo":{"status":"error","timestamp":1619811287895,"user_tz":-180,"elapsed":562172,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}},"outputId":"e02676cd-c48d-4af5-8707-554f79daf42e"},"source":["encoder_shape = (2048, 3, 3)\n","category_count = len(question_types)\n","\n","max_subjects_len = 7\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","first_subject_model_filename = 'subject_model_first_ca_state.pth'\n","second_subject_model_filename = 'subject_model_second_ca_state.pth'\n","category_model_filename = 'category_model_ba_state.pth'\n","\n","\n","encoder = Encoder(encoder_shape=encoder_shape).to(device)\n","\n","category_model = CategoryModel(encoder_shape, 2048, category_count).to(device)\n","category_model.load_state_dict(torch.load(f\"/content/drive/MyDrive/Kool/Ülikool/3. aasta/Lõputöö/{category_model_filename}\"))\n","\n","first_subject_model = SubjectsModel(encoder_shape, category_count, max_subjects_len, cut_off=False).to(device)\n","first_subject_model.load_state_dict(torch.load(f\"/content/drive/MyDrive/Kool/Ülikool/3. aasta/Lõputöö/{first_subject_model_filename}\"))\n","\n","second_subject_model = SubjectsModel(encoder_shape, category_count, max_subjects_len, cut_off=False).to(device)\n","second_subject_model.load_state_dict(torch.load(f\"/content/drive/MyDrive/Kool/Ülikool/3. aasta/Lõputöö/{second_subject_model_filename}\"))\n","\n","\n","category_model.eval()\n","first_subject_model.eval()\n","second_subject_model.eval()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4c4b8d3415846339cd62bce8bc27780","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-b479b0a69ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mfirst_subject_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubjectsModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_subjects_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_off\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfirst_subject_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/content/drive/MyDrive/Kool/Ülikool/3. aasta/Lõputöö/{first_subject_model_filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msecond_subject_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubjectsModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_subjects_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_off\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"qXxkzlbmluJt"},"source":["# loading models and making inferences"]},{"cell_type":"code","metadata":{"id":"RV-KG_i-es7V","executionInfo":{"status":"aborted","timestamp":1619811287889,"user_tz":-180,"elapsed":562161,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["from torch.nn.utils.rnn import pack_padded_sequence\n","from nltk.translate.bleu_score import corpus_bleu\n","import random\n","\n","def subject_model_predict(model, \n","               imgs, \n","               description_type,\n","               opposite_subject_onehot, \n","               subject_lengths):\n","  \n","  # note we are providing them cross. So first element model gets second question type \n","  subject_prediction_scores = model(imgs, \n","                                    description_type, \n","                                    opposite_subject_onehot.float(), \n","                                    subject_lengths=subject_lengths)\n","  \n","  return subject_prediction_scores\n","  \n","\n","\n","def category_model_predict(model, imgs):\n","  category_prediction_scores = model(imgs)\n","  return category_prediction_scores\n","\n","\n","def category_prediction_scores_to_categories(scores, pick_most_confident=True):\n","  scores = scores.cpu().numpy()\n","  pred_threshold = 1/scores.shape[1]\n","\n","  if (pick_most_confident):\n","    predicted_category_indices = [np.argmax(scores)]\n","  else:\n","    predicted_category_indices =  [i for i in np.where(scores > pred_threshold)[1]]\n","  categories_as_strings = [question_types[category_idx] for category_idx in predicted_category_indices]\n","\n","  return predicted_category_indices, categories_as_strings\n","\n","\n","def generate_description(description_type, first_subject, second_subject):\n","  if (description_type == 'GREATER'):\n","    return f\"{first_subject} is greater than {second_subject}\"\n","  if (description_type == 'LESS'):\n","    return f\"{first_subject} is less than {second_subject}\"\n","  if (description_type == 'INTERSECT'):\n","    return f\"{first_subject} intersects {second_subject}\"\n","\n","\n","\n","def subjects_model_predictions_to_subjects(first_subject_prediction_scores, \n","                                           second_subject_prediction_scores, subjects, k=3):\n","  \n","  k = min(len(subjects), k)\n","  # clamp predictions\n","  first_subject_prediction_scores[:, len(subjects):] = 0\n","  second_subject_prediction_scores[:, len(subjects):] = 0\n","\n","\n","  _, top_first_subject_indexes = torch.topk(first_subject_prediction_scores, k)\n","  top_first_subject_indexes = top_first_subject_indexes.cpu().numpy().tolist()[0]\n","\n","  _, top_second_subject_indexes = torch.topk(second_subject_prediction_scores, k)\n","  top_second_subject_indexes = top_second_subject_indexes.cpu().numpy().tolist()[0]\n","\n","  zipped = list(zip(top_first_subject_indexes, top_second_subject_indexes))\n","\n","  subjects_converted = []\n","\n","  for index_pair in zipped:\n","    subjects_converted.append([subjects[index_pair[0]], subjects[index_pair[1]]])\n","\n","  return subjects_converted\n","\n","\n","def infer(batch):\n","  imgs = batch['image'].to(device)\n","  subject_lengths = batch['subject_lengths'].to(device)\n","\n","  imgs = encoder(imgs)\n","\n","  category_prediction_scores = category_model_predict(category_model, imgs)\n","  predicted_category_indices, predicted_category_strings = category_prediction_scores_to_categories(category_prediction_scores, \n","                                                                                                    pick_most_confident=True)\n","\n","  subjects = json.loads(batch['all_subjects_blob'][0])\n","\n","  print(\"Most confident category prediction: \", predicted_category_strings)\n","  plt.figure(figsize = (10, 10))\n","  plt.imshow(batch['original_image'][0])\n","\n","  for idx, category_index in enumerate(predicted_category_indices):\n","    all_description_types = [0]*(len(question_types))\n","    all_description_types[category_index] = 1\n","    description_type = torch.tensor([all_description_types]).float().to(device)\n","\n","    predicted_category_onehot = [0]*(len(question_types))\n","    predicted_category_onehot[category_index] = 1\n","    predicted_category_onehot = torch.tensor(predicted_category_onehot).float().to(device)\n","\n","    predicted_category_onehot = predicted_category_onehot.unsqueeze(0)\n","\n","    first_subject_opposite_onehot = torch.zeros(max_subjects_len).unsqueeze(0).to(device)\n","    first_subject_prediction_scores = subject_model_predict(first_subject_model,\n","                       imgs,\n","                       description_type,\n","                       opposite_subject_onehot=first_subject_opposite_onehot,\n","                       subject_lengths=subject_lengths\n","                       )\n","    \n","\n","    second_subject_opposite_onehot = torch.zeros(max_subjects_len)\n","    predicted_first_subject_index = torch.argmax(first_subject_prediction_scores).clone().cpu().numpy()\n","    second_subject_opposite_onehot[predicted_first_subject_index] = 1\n","    second_subject_opposite_onehot = second_subject_opposite_onehot.unsqueeze(0).to(device)\n","\n","    second_subject_prediction_scores = subject_model_predict(second_subject_model,\n","                       imgs,\n","                       description_type,\n","                       opposite_subject_onehot=second_subject_opposite_onehot,\n","                       subject_lengths=subject_lengths\n","                       )\n","    \n","    top_k_subject_pairs = subjects_model_predictions_to_subjects(first_subject_prediction_scores,\n","                                                                                  second_subject_prediction_scores,\n","                                                                                  subjects, k=3)\n","    \n","    for top_idx, subject_pair in enumerate(top_k_subject_pairs):\n","      category = predicted_category_strings[idx]\n","      print(f\"Model top-{top_idx+1} prediction for {category}: {generate_description(category, subject_pair[0], subject_pair[1])}\")\n","\n","infer_loader = torch.utils.data.DataLoader(\n","        validation_dataset,\n","        batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n","\n","batch = next(iter(val_loader))\n","\n","\n","with torch.no_grad():\n","  infer(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8Waxa5VJBLk","executionInfo":{"status":"aborted","timestamp":1619811287891,"user_tz":-180,"elapsed":562161,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["def descriptions_equal(a, b):\n","  return a[0] == b[0] and a[1] == b[1] and a[2] == b[2]\n","\n","\n","def predicted_description_present(predicted, targets):\n","  for target in targets:\n","    if (descriptions_equal(predicted, target)):\n","      return True\n","\n","  return False\n","\n","\n","top_k_descriptions = 3\n","\n","def validate_all_descriptions(batch):\n","  global all_descriptions_total_count\n","  global all_descriptions_predicted_present_count\n","\n","  imgs = batch['image'].to(device)\n","  subject_lengths = batch['subject_lengths'].to(device)\n","\n","  imgs = encoder(imgs)\n","\n","  category_prediction_scores = category_model_predict(category_model, imgs)\n","  predicted_category_indices, predicted_category_strings = category_prediction_scores_to_categories(category_prediction_scores, \n","                                                                                                    pick_most_confident=False)\n","\n","  subjects = json.loads(batch['all_subjects_blob'][0])\n","  descriptions = json.loads(batch['description_blob'][0])\n","\n","  predicted_descriptions = []\n","\n","  for idx, category_index in enumerate(predicted_category_indices):\n","    all_description_types = [0]*(len(question_types))\n","    all_description_types[category_index] = 1\n","    description_type = torch.tensor([all_description_types]).float().to(device)\n","\n","    predicted_category_onehot = [0]*(len(question_types))\n","    predicted_category_onehot[category_index] = 1\n","    predicted_category_onehot = torch.tensor(predicted_category_onehot).float().to(device)\n","\n","    predicted_category_onehot = predicted_category_onehot.unsqueeze(0)\n","\n","    first_subject_opposite_onehot = torch.zeros(max_subjects_len).unsqueeze(0).to(device)\n","    first_subject_prediction_scores = subject_model_predict(first_subject_model,\n","                       imgs,\n","                       description_type,\n","                       opposite_subject_onehot=first_subject_opposite_onehot,\n","                       subject_lengths=subject_lengths\n","                       )\n","    \n","\n","    second_subject_opposite_onehot = torch.zeros(max_subjects_len)\n","    predicted_first_subject_index = torch.argmax(first_subject_prediction_scores).clone().cpu().numpy()\n","    second_subject_opposite_onehot[predicted_first_subject_index] = 1\n","    second_subject_opposite_onehot = second_subject_opposite_onehot.unsqueeze(0).to(device)\n","\n","    second_subject_prediction_scores = subject_model_predict(second_subject_model,\n","                       imgs,\n","                       description_type,\n","                       opposite_subject_onehot=second_subject_opposite_onehot,\n","                       subject_lengths=subject_lengths\n","                       )\n","    \n","    top_k_subject_pairs = subjects_model_predictions_to_subjects(first_subject_prediction_scores,\n","                                                                 second_subject_prediction_scores,\n","                                                                 subjects, k=top_k_descriptions)\n","\n","    category = predicted_category_strings[idx]      \n","    for pair in top_k_subject_pairs:\n","      predicted_descriptions.append([category, pair[0], pair[1]])\n","\n","    all_descriptions_total_count += 1\n","\n","  for predicted in predicted_descriptions:\n","    if (predicted_description_present(predicted, descriptions)):\n","      all_descriptions_predicted_present_count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODbNj1kEx4Up","executionInfo":{"status":"aborted","timestamp":1619811287891,"user_tz":-180,"elapsed":562159,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["def validate_one_description_per_figure(batch):\n","  global one_description_total_count\n","  global one_description_predicted_present_count\n","\n","  imgs = batch['image'].to(device)\n","  subject_lengths = batch['subject_lengths'].to(device)\n","\n","  imgs = encoder(imgs)\n","\n","  category_prediction_scores = category_model_predict(category_model, imgs)\n","  predicted_category_indices, predicted_category_strings = category_prediction_scores_to_categories(category_prediction_scores, \n","                                                                                                    pick_most_confident=False)\n","\n","  subjects = json.loads(batch['all_subjects_blob'][0])\n","  descriptions = json.loads(batch['description_blob'][0])\n","\n","  predicted_descriptions = []\n","\n","  for idx, category_index in enumerate(predicted_category_indices):\n","    all_description_types = [0]*(len(question_types))\n","    all_description_types[category_index] = 1\n","    description_type = torch.tensor([all_description_types]).float().to(device)\n","\n","    predicted_category_onehot = [0]*(len(question_types))\n","    predicted_category_onehot[category_index] = 1\n","    predicted_category_onehot = torch.tensor(predicted_category_onehot).float().to(device)\n","\n","    predicted_category_onehot = predicted_category_onehot.unsqueeze(0)\n","\n","    first_subject_opposite_onehot = torch.zeros(max_subjects_len).unsqueeze(0).to(device)\n","    first_subject_prediction_scores = subject_model_predict(first_subject_model,\n","                       imgs,\n","                       description_type,\n","                       opposite_subject_onehot=first_subject_opposite_onehot,\n","                       subject_lengths=subject_lengths\n","                       )\n","    \n","\n","    second_subject_opposite_onehot = torch.zeros(max_subjects_len)\n","    predicted_first_subject_index = torch.argmax(first_subject_prediction_scores).clone().cpu().numpy()\n","    second_subject_opposite_onehot[predicted_first_subject_index] = 1\n","    second_subject_opposite_onehot = second_subject_opposite_onehot.unsqueeze(0).to(device)\n","\n","    second_subject_prediction_scores = subject_model_predict(second_subject_model,\n","                       imgs,\n","                       description_type,\n","                       opposite_subject_onehot=second_subject_opposite_onehot,\n","                       subject_lengths=subject_lengths\n","                       )\n","    \n","    top_k_subject_pairs = subjects_model_predictions_to_subjects(first_subject_prediction_scores,\n","                                                                 second_subject_prediction_scores,\n","                                                                 subjects, k=top_k_descriptions)\n","\n","    category = predicted_category_strings[idx]      \n","    for pair in top_k_subject_pairs:\n","      predicted_descriptions.append([category, pair[0], pair[1]])\n","\n","  one_description_total_count += 1\n","\n","  for predicted in predicted_descriptions:\n","    if (predicted_description_present(predicted, descriptions)):\n","      one_description_predicted_present_count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GAo8Ty3zH8u","executionInfo":{"status":"aborted","timestamp":1619811287892,"user_tz":-180,"elapsed":562159,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["def validate_only_subjects(batch):\n","  global only_subjects_total_count\n","  global only_subjects_predicted_present_count\n","\n","  imgs = batch['image'].to(device)\n","  subject_lengths = batch['subject_lengths'].to(device)\n","\n","  imgs = encoder(imgs)\n","  subjects = json.loads(batch['all_subjects_blob'][0])\n","  descriptions = json.loads(batch['description_blob'][0])\n","\n","  predicted_category_indices = [question_types.index(description[0]) for description in descriptions]\n","  predicted_category_strings = [description[0] for description in descriptions]\n","\n","  predicted_descriptions = []\n","\n","  for idx, category_index in enumerate(predicted_category_indices):\n","    all_description_types = [0]*(len(question_types))\n","    all_description_types[category_index] = 1\n","    description_type = torch.tensor([all_description_types]).float().to(device)\n","\n","    predicted_category_onehot = [0]*(len(question_types))\n","    predicted_category_onehot[category_index] = 1\n","    predicted_category_onehot = torch.tensor(predicted_category_onehot).float().to(device)\n","\n","    predicted_category_onehot = predicted_category_onehot.unsqueeze(0)\n","\n","    first_subject_opposite_onehot = torch.zeros(max_subjects_len).unsqueeze(0).to(device)\n","    first_subject_prediction_scores = subject_model_predict(first_subject_model,\n","                       imgs,\n","                       description_type,\n","                       opposite_subject_onehot=first_subject_opposite_onehot,\n","                       subject_lengths=subject_lengths\n","                       )\n","    \n","\n","    second_subject_opposite_onehot = torch.zeros(max_subjects_len)\n","    predicted_first_subject_index = torch.argmax(first_subject_prediction_scores).clone().cpu().numpy()\n","    second_subject_opposite_onehot[predicted_first_subject_index] = 1\n","    second_subject_opposite_onehot = second_subject_opposite_onehot.unsqueeze(0).to(device)\n","\n","    second_subject_prediction_scores = subject_model_predict(second_subject_model,\n","                       imgs,\n","                       description_type,\n","                       opposite_subject_onehot=second_subject_opposite_onehot,\n","                       subject_lengths=subject_lengths\n","                       )\n","    \n","    top_k_subject_pairs = subjects_model_predictions_to_subjects(first_subject_prediction_scores,\n","                                                                 second_subject_prediction_scores,\n","                                                                 subjects, k=top_k_descriptions)\n","\n","    category = predicted_category_strings[idx]      \n","    for pair in top_k_subject_pairs:\n","      predicted_descriptions.append([category, pair[0], pair[1]])\n","\n","    only_subjects_total_count += 1\n","\n","  for predicted in predicted_descriptions:\n","    if (predicted_description_present(predicted, descriptions)):\n","      only_subjects_predicted_present_count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2Y50v3zL9Df","executionInfo":{"status":"aborted","timestamp":1619811287893,"user_tz":-180,"elapsed":562155,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["only_subjects_accuracies = []\n","one_description_accuracies = []\n","all_descriptions_accuracies = []\n","\n","\n","\n","k_list = [1, 2, 3, 4]\n","for top_k_descriptions in k_list:\n","  print(f\"k-{top_k_descriptions}\")\n","  val_loader = torch.utils.data.DataLoader(\n","        validation_dataset,\n","        batch_size=1, shuffle=True, num_workers=1)\n","\n","  only_subjects_predicted_present_count = 0\n","  only_subjects_total_count = 0\n","\n","  one_description_predicted_present_count = 0\n","  one_description_total_count = 0\n","\n","  all_descriptions_predicted_present_count = 0\n","  all_descriptions_total_count = 0\n","\n","  with torch.no_grad():\n","    for batch in val_loader:\n","      try:\n","        validate_only_subjects(batch)\n","      except:\n","        pass\n","\n","      try:\n","        validate_one_description_per_figure(batch)\n","      except:\n","        pass\n","    \n","      try:\n","        validate_all_descriptions(batch)\n","      except:\n","        pass\n","  \n","  print(only_subjects_predicted_present_count, only_subjects_total_count)\n","  print(one_description_predicted_present_count, one_description_total_count)\n","  print(all_descriptions_predicted_present_count, all_descriptions_total_count)\n","\n","  only_subjects_accuracies.append(only_subjects_predicted_present_count / only_subjects_total_count)\n","  one_description_accuracies.append(one_description_predicted_present_count / one_description_total_count)\n","  all_descriptions_accuracies.append(all_descriptions_predicted_present_count / all_descriptions_total_count)\n","\n","\n","\n","print(only_subjects_accuracies)\n","print(one_description_accuracies)\n","print(all_descriptions_accuracies)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pgkn2LBwo6_l","executionInfo":{"status":"aborted","timestamp":1619811287893,"user_tz":-180,"elapsed":562149,"user":{"displayName":"Sander Nemvalts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj15RlhhDJLopIRnXC5Oemrjl0ZUt15YL3tp0Brog=s64","userId":"10729714349318498224"}}},"source":["import matplotlib.pyplot as plt\n","figure = plt.figure(figsize=(10,7.5))\n","axis = plt.axes()\n","\n","\n","baseline_accuracy = 1/len(question_types) * 1/4 * 1/3\n","\n","plt.title('Accuracies of different evaluation variations')\n","plt.xlabel('k in top-k')\n","plt.ylabel('Accuracy')\n","plt.xticks(k_list)\n","\n","one_descriptions_line, = axis.plot(k_list, one_description_accuracies)\n","all_descriptions_line, = axis.plot(k_list, all_descriptions_accuracies)\n","only_subjects_line, = axis.plot(k_list, only_subjects_accuracies)\n","\n","baseline_accuracy_line, = axis.plot(k_list, [(k+1) * acc for k, acc in enumerate(len(k_list) * [baseline_accuracy])], 'k--')\n","\n","one_descriptions_line.set_label('First variation accuracy')\n","all_descriptions_line.set_label('Second variation accuracy')\n","only_subjects_line.set_label('Third variation accuracy')\n","baseline_accuracy_line.set_label('Baseline (random pick) accuracy')\n","axis.legend()\n","#axis.legend([only_subjects_line], ['Only subjects'])"],"execution_count":null,"outputs":[]}]}