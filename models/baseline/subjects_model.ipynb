{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4. Resnet + subjects model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "edb5d169f2ac4e90a0347591c6d676cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce44614db59c406fb8c4488e86ee2691",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8d7792f518904498bfc5a7f4650f147c",
              "IPY_MODEL_1403ef83119949cfa9ac157909230344"
            ]
          }
        },
        "ce44614db59c406fb8c4488e86ee2691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d7792f518904498bfc5a7f4650f147c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e049f11b4b7e48c4bfe3e3072cec5c24",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5be9755319ef4ba68e2720df1c63ed4d"
          }
        },
        "1403ef83119949cfa9ac157909230344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08a3cc22abf04417bf571491107463ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:18&lt;00:00, 5.43MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e415710785241e4bd492af6891a3ad3"
          }
        },
        "e049f11b4b7e48c4bfe3e3072cec5c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5be9755319ef4ba68e2720df1c63ed4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08a3cc22abf04417bf571491107463ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e415710785241e4bd492af6891a3ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnhyS1VLNrfh"
      },
      "source": [
        "# Starting out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAoN8e62MAif",
        "outputId": "5b588dfd-0e03-4166-e806-3b137a060bc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wncr2153ROgV"
      },
      "source": [
        "!cp /content/drive/MyDrive/Kool/Ãœlikool/'3. aasta'/LÃµputÃ¶Ã¶/data.zip .\n",
        "!unzip -q data.zip\n",
        "!rm data.zip\n",
        "\n",
        "!cp /content/drive/MyDrive/Kool/Ãœlikool/'3. aasta'/LÃµputÃ¶Ã¶/test_val.zip .\n",
        "!unzip -q test_val.zip\n",
        "!rm test_val.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RgggMjkYEDc",
        "outputId": "e2fe816f-1280-47ad-f495-bd5fdc37b401"
      },
      "source": [
        "!rm -rf line-chart-captioning/\n",
        "#clone repo\n",
        "!git clone https://github.com/snemvalts/line-chart-captioning\n",
        "#clean out data folder and move extracted raw data to captioning\n",
        "!rm -rf line-chart-captioning/data/*\n",
        "!mv data/** line-chart-captioning/data/\n",
        "!mv test_val/** line-chart-captioning/data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'line-chart-captioning'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 129 (delta 40), reused 101 (delta 22), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (129/129), 22.30 KiB | 7.43 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjFeVDn2_pEo"
      },
      "source": [
        "import json\n",
        "\n",
        "# Smoothest and roughest seem to always appear. Same for MIN_AUC and MAX_AUC\n",
        "question_types = [\"GREATER\", \"LESS\", \"INTERSECT\"]\n",
        "synthetic_json = {\n",
        "  \"questions\": question_types\n",
        "}\n",
        "\n",
        "with open('line-chart-captioning/synthetic.json', 'w') as f:\n",
        "  json.dump(synthetic_json, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoZgEECwhGVe",
        "outputId": "fa43d95d-ef73-40f2-e130-c6d4b361dbb3"
      },
      "source": [
        "!cd line-chart-captioning && \\\n",
        "python3 src/synthetic/preprocess-question-types.py \\\n",
        "--synthetic-config synthetic.json \\\n",
        "--unroll \\\n",
        "data/figureqa/train1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parsing QA...\n",
            "parsing annotations...\n",
            "processing plots...\n",
            "copying images...\n",
            "writing csv...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg4SToRdybsI"
      },
      "source": [
        "## creating transforms & dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQu5ntlOoscT"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from skimage import transform\n",
        "\n",
        "\n",
        "# adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "class SyntheticImageDataset(Dataset):\n",
        "  def __init__(self, csv_file, images_dir, transform=None):\n",
        "    self.charts_captions = pd.read_csv(csv_file)\n",
        "    self.images_dir = images_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.charts_captions)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    [image_number, description_blob, all_subjects_blob] = self.charts_captions.iloc[idx]\n",
        "    image_path = os.path.join(self.images_dir, f'{str(image_number)}.png')\n",
        "    image = np.array(Image.open(image_path))\n",
        "    \n",
        "    sample = {\n",
        "        'image': image, \n",
        "        'description_blob': description_blob,\n",
        "        'all_subjects_blob': all_subjects_blob   \n",
        "    }\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o2x0Jb8MDdM"
      },
      "source": [
        "class ResizeImage(object):\n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    resized_image = transform.resize(sample['image'], self.output_size)\n",
        "    return {**sample, 'image': resized_image}\n",
        "\n",
        "class StripImageTransparency(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    stripped_transparency_image = sample['image'][:,:,:3]\n",
        "    return {**sample, 'image': stripped_transparency_image}\n",
        "\n",
        "\n",
        "class NormalizeImage(object):\n",
        "  def __init__(self, mean, std):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    normalized_image = (sample['image'] - self.mean) / self.std\n",
        "    return {**sample, 'image': normalized_image}\n",
        "\n",
        "\n",
        "class ImageToTensor(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    image = sample['image'].transpose((2, 0, 1))\n",
        "    return {**sample, 'image': torch.tensor(image).float()}\n",
        "\n",
        "\n",
        "class FirstQuestionTypeToOneHotTensor(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    descriptions = json.loads(sample['description_blob'])\n",
        "    first_description = descriptions[0]\n",
        "    first_description_type = first_description[0]\n",
        "\n",
        "    all_description_types = [0]*(len(question_types))\n",
        "\n",
        "    description_type_index = question_types.index(first_description_type)\n",
        "    all_description_types[description_type_index] = 1\n",
        "\n",
        "    return {**sample, 'first_description_type': torch.tensor(all_description_types).float() }\n",
        "\n",
        "\n",
        "class IncludeSubjectLengths(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    all_subjects_len = len(json.loads(sample['all_subjects_blob']))\n",
        "    return {**sample, 'subject_lengths': all_subjects_len }\n",
        "\n",
        "\n",
        "class FirstQuestionSubjectsToOneHotTensor(object):\n",
        "  def __init__(self, max_len):\n",
        "    self.max_len = max_len\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    descriptions = json.loads(sample['description_blob'])\n",
        "    all_subjects = json.loads(sample['all_subjects_blob'])\n",
        "\n",
        "    first_description = descriptions[0]\n",
        "    first_description_first_subject = first_description[1]\n",
        "    first_description_second_subject = first_description[2]\n",
        "\n",
        "    first_subject_onehot = [0]*self.max_len\n",
        "    first_description_subject_index = all_subjects.index(first_description_first_subject)\n",
        "    first_subject_onehot[first_description_subject_index] = 1\n",
        "\n",
        "    second_subject_onehot = [0]*self.max_len\n",
        "    second_description_subject_index = all_subjects.index(first_description_second_subject)\n",
        "    second_subject_onehot[second_description_subject_index] = 1\n",
        "\n",
        "    return {\n",
        "        **sample, \n",
        "        'first_description_subject_onehot': torch.tensor(first_subject_onehot).long(),\n",
        "        'first_description_subject_index': first_description_subject_index,\n",
        "        'second_description_subject_onehot': torch.tensor(second_subject_onehot).long(),\n",
        "        'second_description_subject_index': second_description_subject_index\n",
        "    }\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFYGum8CMi4h"
      },
      "source": [
        "import torch\n",
        "\n",
        "def get_dataset(images_dir = None, csv_file = None):\n",
        "  # parallely calculate max caption len and word map to pass it to padcaption\n",
        "  # cause can't access dataset from transforms before it is defined in compose\n",
        "\n",
        "  captions_csv = pd.read_csv(csv_file)\n",
        "  \n",
        "  max_subjects_len = captions_csv['all_subjects_blob'].apply(lambda blob: len(json.loads(blob))).max()\n",
        "  \n",
        "\n",
        "  dataset = SyntheticImageDataset(images_dir = images_dir,\n",
        "                           csv_file = csv_file,\n",
        "                           transform=transforms.Compose([\n",
        "                                  ResizeImage((224, 224)),\n",
        "                                  StripImageTransparency(),\n",
        "                                  NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                                  ImageToTensor(),\n",
        "                                  FirstQuestionTypeToOneHotTensor(),\n",
        "                                  FirstQuestionSubjectsToOneHotTensor(max_subjects_len),\n",
        "                                  IncludeSubjectLengths()\n",
        "                                 ]))\n",
        "\n",
        "  return dataset, max_subjects_len\n",
        "\n",
        "\n",
        "train_dataset, max_subjects_len = get_dataset(images_dir = 'line-chart-captioning/data/processed_synthetic/train1-types/images',\n",
        "                      csv_file = 'line-chart-captioning/data/processed_synthetic/train1-types/captions.csv')\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VNbi6_FykrD"
      },
      "source": [
        "## encoder and elements model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCMmvlNbknM1"
      },
      "source": [
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, encoder_shape):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    base_resnet = torchvision.models.resnet50(pretrained=True) \n",
        "    resnet_without_fc = nn.Sequential(*(list(base_resnet.children())[:-2]))\n",
        "\n",
        "    # freeze weights of resnet \n",
        "    for parameter in resnet_without_fc.parameters():\n",
        "      parameter.requires_grad = False\n",
        "\n",
        "    self.resnet = resnet_without_fc\n",
        "    self.pool = nn.AdaptiveAvgPool3d(encoder_shape)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.resnet(x)\n",
        "    x = self.pool(x)\n",
        "    x = torch.reshape(x, (-1, 2048*3*3))\n",
        "    return x\n",
        "\n",
        "class SubjectsModel(nn.Module):\n",
        "  def __init__(self, encoder_shape, \n",
        "               category_count,\n",
        "               subject_max_len, \n",
        "               cut_off=True):\n",
        "    super(SubjectsModel, self).__init__()\n",
        "\n",
        "    encoder_dim = 1\n",
        "    for dim in encoder_shape:\n",
        "      encoder_dim *= dim\n",
        "\n",
        "    self.question_type_dense = nn.Linear(category_count, 1024)\n",
        "    self.other_subject_dense = nn.Linear(subject_max_len, 1024)\n",
        "    self.encoder_dense = nn.Linear(encoder_dim, 1024)\n",
        "\n",
        "    hidden_dim = 1024 * 3\n",
        "\n",
        "    self.hidden1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.hidden3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.hidden4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.hidden5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    self.output = nn.Linear(hidden_dim, subject_max_len)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.cut_off = cut_off\n",
        "    self.softmax = nn.Softmax()\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    for m in [self.question_type_dense, self.other_subject_dense, self.encoder_dense,\n",
        "              self.hidden1, self.hidden2, self.hidden3, self.hidden4, self.hidden5]:\n",
        "      torch.nn.init.xavier_uniform(m.weight)\n",
        "      m.bias.data.fill_(0.01)\n",
        "\n",
        "  def forward(self, encoder_out, question_type_onehot, other_subject_onehot, other_subject_indices=[], subject_lengths=[]):\n",
        "    batch_size = encoder_out.shape[0]\n",
        "\n",
        "    x_category = self.question_type_dense(question_type_onehot)\n",
        "    x_subject = self.other_subject_dense(other_subject_onehot)\n",
        "    x_encoder = self.encoder_dense(encoder_out)\n",
        "\n",
        "    x = torch.cat((x_category, x_subject, x_encoder), dim=1)\n",
        "\n",
        "    x = self.relu(self.hidden1(x))\n",
        "    x = self.relu(self.hidden2(x))\n",
        "    x = self.relu(self.hidden3(x))\n",
        "\n",
        "    x = self.relu(self.hidden4(x))\n",
        "    x = self.relu(self.hidden5(x))\n",
        "\n",
        "    x = self.output(x)\n",
        "    #print(\"mean x before softmax\", x.mean())\n",
        "    x = self.softmax(x)\n",
        "\n",
        "\n",
        "    x_cloned = x.clone()\n",
        "    # if cut off is enabled, use subject lengths to set any unnecessary predictions to 0\n",
        "    if (self.cut_off):\n",
        "        assert len(subject_lengths) == batch_size, 'Subject lengths not provided'\n",
        "        for i, length in enumerate(subject_lengths):\n",
        "          #print('before', x_cloned[i], 'with length ', length)\n",
        "          x_cloned[i, length:] = 0.0\n",
        "          \n",
        "\n",
        "        # reset the other subject prediction to 0 as well \n",
        "        for i, other_subject_index in enumerate(other_subject_indices):\n",
        "          x_cloned[i][other_subject_index] = 0.0\n",
        "\n",
        "    return x_cloned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXxkzlbmluJt"
      },
      "source": [
        "# training the elements model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "edb5d169f2ac4e90a0347591c6d676cd",
            "ce44614db59c406fb8c4488e86ee2691",
            "8d7792f518904498bfc5a7f4650f147c",
            "1403ef83119949cfa9ac157909230344",
            "e049f11b4b7e48c4bfe3e3072cec5c24",
            "5be9755319ef4ba68e2720df1c63ed4d",
            "08a3cc22abf04417bf571491107463ac",
            "6e415710785241e4bd492af6891a3ad3"
          ]
        },
        "id": "RV-KG_i-es7V",
        "outputId": "25fa0948-aa14-4095-bc19-f2a8490ee7c7"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import random\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "def accuracy(prediction_scores, target):\n",
        "  predictions = torch.argmax(prediction_scores, dim=1)\n",
        "  correct = torch.eq(predictions, target).sum()\n",
        "  correct = correct.sum()\n",
        "  total = target.shape[0]\n",
        "  return (correct/total)*100.0\n",
        "\n",
        "\n",
        "def model_step(model, \n",
        "               optimizer, \n",
        "               imgs, \n",
        "               description_type,\n",
        "               target_subjects_index,\n",
        "               opposite_subject_onehot, \n",
        "               opposite_subject_index,\n",
        "               subject_lengths, \n",
        "               criterion):\n",
        "  # note we are providing them cross. So first element model gets second question type \n",
        "  subject_prediction_scores = model(imgs, \n",
        "                                    description_type, \n",
        "                                    opposite_subject_onehot.float(), \n",
        "                                    opposite_subject_index, \n",
        "                                    subject_lengths=subject_lengths)\n",
        "\n",
        "  # note we are providing them cross. So first element \n",
        "  loss = criterion(subject_prediction_scores, target_subjects_index)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return subject_prediction_scores, loss\n",
        "\n",
        "def train_epoch(iter_cap=50):\n",
        "  global min_loss\n",
        "  iter_without_improvement = 0\n",
        "\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    imgs = batch['image'].to(device)\n",
        "    description_type = batch['first_description_type'].to(device)\n",
        "    subject_lengths = batch['subject_lengths']\n",
        "\n",
        "    first_description_first_subject = batch['first_description_subject_onehot'].to(device)\n",
        "    first_description_first_subject_index = batch['first_description_subject_index'].to(device)\n",
        "\n",
        "    first_description_second_subject = batch['second_description_subject_onehot'].to(device)\n",
        "    first_description_second_subject_index = batch['second_description_subject_index'].to(device)\n",
        "\n",
        "    imgs = encoder(imgs)\n",
        "\n",
        "    first_subject_prediction_scores, loss_first = model_step(first_subject_model,\n",
        "                       first_subject_model_optimizer,\n",
        "                       imgs,\n",
        "                       description_type,\n",
        "                       target_subjects_index=first_description_first_subject_index,\n",
        "                       opposite_subject_onehot=first_description_second_subject,\n",
        "                       opposite_subject_index=first_description_second_subject_index,\n",
        "                       subject_lengths=subject_lengths,\n",
        "                       criterion=first_criterion\n",
        "                       )\n",
        "    \n",
        "    second_subject_prediction_scores, loss_second = model_step(second_subject_model,\n",
        "                       second_subject_model_optimizer,\n",
        "                       imgs,\n",
        "                       description_type,\n",
        "                       target_subjects_index=first_description_second_subject_index,\n",
        "                       opposite_subject_onehot=first_description_first_subject,\n",
        "                       opposite_subject_index=first_description_first_subject_index,\n",
        "                       subject_lengths=subject_lengths,\n",
        "                       criterion=second_criterion\n",
        "                       )\n",
        "    \n",
        "    #encoder_optimizer.zero_grad()\n",
        "    #encoder_optimizer.step()\n",
        "\n",
        "\n",
        "    loss_score = (loss_first + loss_second).cpu().detach().numpy()\n",
        "\n",
        "    if (loss_score <= min_loss):\n",
        "      min_loss = loss_score\n",
        "      iter_without_improvement = 0\n",
        "    else:\n",
        "      iter_without_improvement += 1\n",
        "\n",
        "    accuracy_score_first_subject = accuracy(first_subject_prediction_scores, first_description_first_subject_index)\n",
        "    accuracy_score_second_subject = accuracy(second_subject_prediction_scores, first_description_second_subject_index)\n",
        "\n",
        "    accuracies[0].append(accuracy_score_first_subject)\n",
        "    accuracies[1].append(accuracy_score_second_subject)\n",
        "    losses.append(loss_score)\n",
        "\n",
        "    print(f\"Batch #{i}/{len(train_loader)}: Loss is {loss_score:.3f}\")\n",
        "    print(f\"Accuracy for first subject is {accuracy_score_first_subject:.2f}%, for second subject {accuracy_score_second_subject:.2f}%\")\n",
        "\n",
        "    if (iter_without_improvement >= iter_cap):\n",
        "      print(f\"{iter_cap} iterations without improvement. stopping\")\n",
        "      break\n",
        "\n",
        "\n",
        "\n",
        "encoder_shape = (2048, 3, 3)\n",
        "category_count = len(question_types)\n",
        "\n",
        "subjects_model_lr = 6e-5\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "accuracies = [[], []]\n",
        "losses = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = Encoder(encoder_shape=encoder_shape).to(device)\n",
        "#encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "#                                             lr=encoder_lr)\n",
        "\n",
        "first_subject_model = SubjectsModel(encoder_shape, category_count, max_subjects_len).to(device)\n",
        "first_subject_model_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, first_subject_model.parameters()),\n",
        "                                             lr=subjects_model_lr)\n",
        "\n",
        "second_subject_model = SubjectsModel(encoder_shape, category_count, max_subjects_len).to(device)\n",
        "second_subject_model_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, second_subject_model.parameters()),\n",
        "                                             lr=subjects_model_lr)\n",
        "\n",
        "first_criterion = nn.CrossEntropyLoss().to(device)\n",
        "second_criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "min_loss = float('inf')\n",
        "iter_without_improvement = 0\n",
        "\n",
        "first_subject_model.train()\n",
        "second_subject_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch + 1} of {epochs}\")\n",
        "  train_epoch(iter_cap = 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edb5d169f2ac4e90a0347591c6d676cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch #0/1433: Loss is 3.764\n",
            "Accuracy for first subject is 43.75%, for second subject 37.50%\n",
            "Batch #1/1433: Loss is 3.755\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #2/1433: Loss is 3.758\n",
            "Accuracy for first subject is 28.12%, for second subject 31.25%\n",
            "Batch #3/1433: Loss is 3.735\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #4/1433: Loss is 3.752\n",
            "Accuracy for first subject is 28.12%, for second subject 31.25%\n",
            "Batch #5/1433: Loss is 3.861\n",
            "Accuracy for first subject is 18.75%, for second subject 18.75%\n",
            "Batch #6/1433: Loss is 3.758\n",
            "Accuracy for first subject is 37.50%, for second subject 21.88%\n",
            "Batch #7/1433: Loss is 3.745\n",
            "Accuracy for first subject is 34.38%, for second subject 34.38%\n",
            "Batch #8/1433: Loss is 3.765\n",
            "Accuracy for first subject is 34.38%, for second subject 15.62%\n",
            "Batch #9/1433: Loss is 3.726\n",
            "Accuracy for first subject is 34.38%, for second subject 25.00%\n",
            "Batch #10/1433: Loss is 3.524\n",
            "Accuracy for first subject is 37.50%, for second subject 46.88%\n",
            "Batch #11/1433: Loss is 3.675\n",
            "Accuracy for first subject is 50.00%, for second subject 46.88%\n",
            "Batch #12/1433: Loss is 3.744\n",
            "Accuracy for first subject is 21.88%, for second subject 37.50%\n",
            "Batch #13/1433: Loss is 3.917\n",
            "Accuracy for first subject is 31.25%, for second subject 25.00%\n",
            "Batch #14/1433: Loss is 3.817\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #15/1433: Loss is 3.652\n",
            "Accuracy for first subject is 31.25%, for second subject 40.62%\n",
            "Batch #16/1433: Loss is 3.738\n",
            "Accuracy for first subject is 40.62%, for second subject 28.12%\n",
            "Batch #17/1433: Loss is 3.720\n",
            "Accuracy for first subject is 53.12%, for second subject 34.38%\n",
            "Batch #18/1433: Loss is 3.727\n",
            "Accuracy for first subject is 40.62%, for second subject 40.62%\n",
            "Batch #19/1433: Loss is 3.743\n",
            "Accuracy for first subject is 34.38%, for second subject 40.62%\n",
            "Batch #20/1433: Loss is 3.800\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #21/1433: Loss is 3.742\n",
            "Accuracy for first subject is 37.50%, for second subject 25.00%\n",
            "Batch #22/1433: Loss is 3.619\n",
            "Accuracy for first subject is 50.00%, for second subject 37.50%\n",
            "Batch #23/1433: Loss is 3.826\n",
            "Accuracy for first subject is 28.12%, for second subject 25.00%\n",
            "Batch #24/1433: Loss is 3.756\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #25/1433: Loss is 3.695\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #26/1433: Loss is 3.696\n",
            "Accuracy for first subject is 50.00%, for second subject 40.62%\n",
            "Batch #27/1433: Loss is 3.597\n",
            "Accuracy for first subject is 46.88%, for second subject 50.00%\n",
            "Batch #28/1433: Loss is 3.668\n",
            "Accuracy for first subject is 43.75%, for second subject 40.62%\n",
            "Batch #29/1433: Loss is 3.745\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #30/1433: Loss is 3.654\n",
            "Accuracy for first subject is 46.88%, for second subject 43.75%\n",
            "Batch #31/1433: Loss is 3.749\n",
            "Accuracy for first subject is 31.25%, for second subject 40.62%\n",
            "Batch #32/1433: Loss is 3.738\n",
            "Accuracy for first subject is 31.25%, for second subject 28.12%\n",
            "Batch #33/1433: Loss is 3.627\n",
            "Accuracy for first subject is 50.00%, for second subject 28.12%\n",
            "Batch #34/1433: Loss is 3.579\n",
            "Accuracy for first subject is 40.62%, for second subject 46.88%\n",
            "Batch #35/1433: Loss is 3.887\n",
            "Accuracy for first subject is 34.38%, for second subject 25.00%\n",
            "Batch #36/1433: Loss is 3.781\n",
            "Accuracy for first subject is 31.25%, for second subject 31.25%\n",
            "Batch #37/1433: Loss is 3.576\n",
            "Accuracy for first subject is 50.00%, for second subject 43.75%\n",
            "Batch #38/1433: Loss is 3.786\n",
            "Accuracy for first subject is 40.62%, for second subject 25.00%\n",
            "Batch #39/1433: Loss is 3.899\n",
            "Accuracy for first subject is 25.00%, for second subject 21.88%\n",
            "Batch #40/1433: Loss is 3.640\n",
            "Accuracy for first subject is 31.25%, for second subject 46.88%\n",
            "Batch #41/1433: Loss is 3.799\n",
            "Accuracy for first subject is 31.25%, for second subject 31.25%\n",
            "Batch #42/1433: Loss is 3.743\n",
            "Accuracy for first subject is 37.50%, for second subject 21.88%\n",
            "Batch #43/1433: Loss is 3.631\n",
            "Accuracy for first subject is 46.88%, for second subject 34.38%\n",
            "Batch #44/1433: Loss is 3.637\n",
            "Accuracy for first subject is 37.50%, for second subject 46.88%\n",
            "Batch #45/1433: Loss is 3.629\n",
            "Accuracy for first subject is 31.25%, for second subject 43.75%\n",
            "Batch #46/1433: Loss is 3.632\n",
            "Accuracy for first subject is 43.75%, for second subject 34.38%\n",
            "Batch #47/1433: Loss is 3.709\n",
            "Accuracy for first subject is 34.38%, for second subject 37.50%\n",
            "Batch #48/1433: Loss is 3.709\n",
            "Accuracy for first subject is 31.25%, for second subject 25.00%\n",
            "Batch #49/1433: Loss is 3.772\n",
            "Accuracy for first subject is 21.88%, for second subject 28.12%\n",
            "Batch #50/1433: Loss is 3.776\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #51/1433: Loss is 3.788\n",
            "Accuracy for first subject is 37.50%, for second subject 28.12%\n",
            "Batch #52/1433: Loss is 3.732\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #53/1433: Loss is 3.734\n",
            "Accuracy for first subject is 34.38%, for second subject 28.12%\n",
            "Batch #54/1433: Loss is 3.793\n",
            "Accuracy for first subject is 28.12%, for second subject 25.00%\n",
            "Batch #55/1433: Loss is 3.662\n",
            "Accuracy for first subject is 34.38%, for second subject 43.75%\n",
            "Batch #56/1433: Loss is 3.767\n",
            "Accuracy for first subject is 34.38%, for second subject 28.12%\n",
            "Batch #57/1433: Loss is 3.640\n",
            "Accuracy for first subject is 37.50%, for second subject 40.62%\n",
            "Batch #58/1433: Loss is 3.536\n",
            "Accuracy for first subject is 59.38%, for second subject 46.88%\n",
            "Batch #59/1433: Loss is 3.757\n",
            "Accuracy for first subject is 25.00%, for second subject 31.25%\n",
            "Batch #60/1433: Loss is 3.700\n",
            "Accuracy for first subject is 28.12%, for second subject 37.50%\n",
            "Batch #61/1433: Loss is 3.727\n",
            "Accuracy for first subject is 28.12%, for second subject 37.50%\n",
            "Batch #62/1433: Loss is 3.610\n",
            "Accuracy for first subject is 37.50%, for second subject 40.62%\n",
            "Batch #63/1433: Loss is 3.744\n",
            "Accuracy for first subject is 40.62%, for second subject 21.88%\n",
            "Batch #64/1433: Loss is 3.657\n",
            "Accuracy for first subject is 28.12%, for second subject 46.88%\n",
            "Batch #65/1433: Loss is 3.674\n",
            "Accuracy for first subject is 43.75%, for second subject 31.25%\n",
            "Batch #66/1433: Loss is 3.630\n",
            "Accuracy for first subject is 37.50%, for second subject 43.75%\n",
            "Batch #67/1433: Loss is 3.625\n",
            "Accuracy for first subject is 43.75%, for second subject 43.75%\n",
            "Batch #68/1433: Loss is 3.670\n",
            "Accuracy for first subject is 40.62%, for second subject 34.38%\n",
            "Batch #69/1433: Loss is 3.768\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #70/1433: Loss is 3.719\n",
            "Accuracy for first subject is 25.00%, for second subject 34.38%\n",
            "Batch #71/1433: Loss is 3.716\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #72/1433: Loss is 3.642\n",
            "Accuracy for first subject is 25.00%, for second subject 40.62%\n",
            "Batch #73/1433: Loss is 3.578\n",
            "Accuracy for first subject is 43.75%, for second subject 46.88%\n",
            "Batch #74/1433: Loss is 3.698\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #75/1433: Loss is 3.754\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #76/1433: Loss is 3.836\n",
            "Accuracy for first subject is 28.12%, for second subject 21.88%\n",
            "Batch #77/1433: Loss is 3.719\n",
            "Accuracy for first subject is 34.38%, for second subject 37.50%\n",
            "Batch #78/1433: Loss is 3.710\n",
            "Accuracy for first subject is 25.00%, for second subject 28.12%\n",
            "Batch #79/1433: Loss is 3.819\n",
            "Accuracy for first subject is 25.00%, for second subject 37.50%\n",
            "Batch #80/1433: Loss is 3.718\n",
            "Accuracy for first subject is 34.38%, for second subject 28.12%\n",
            "Batch #81/1433: Loss is 3.745\n",
            "Accuracy for first subject is 34.38%, for second subject 28.12%\n",
            "Batch #82/1433: Loss is 3.685\n",
            "Accuracy for first subject is 34.38%, for second subject 40.62%\n",
            "Batch #83/1433: Loss is 3.786\n",
            "Accuracy for first subject is 31.25%, for second subject 25.00%\n",
            "Batch #84/1433: Loss is 3.827\n",
            "Accuracy for first subject is 21.88%, for second subject 25.00%\n",
            "Batch #85/1433: Loss is 3.568\n",
            "Accuracy for first subject is 37.50%, for second subject 50.00%\n",
            "Batch #86/1433: Loss is 3.747\n",
            "Accuracy for first subject is 25.00%, for second subject 25.00%\n",
            "Batch #87/1433: Loss is 3.640\n",
            "Accuracy for first subject is 28.12%, for second subject 40.62%\n",
            "Batch #88/1433: Loss is 3.639\n",
            "Accuracy for first subject is 31.25%, for second subject 43.75%\n",
            "Batch #89/1433: Loss is 3.747\n",
            "Accuracy for first subject is 34.38%, for second subject 25.00%\n",
            "Batch #90/1433: Loss is 3.750\n",
            "Accuracy for first subject is 15.62%, for second subject 25.00%\n",
            "Batch #91/1433: Loss is 3.691\n",
            "Accuracy for first subject is 25.00%, for second subject 37.50%\n",
            "Batch #92/1433: Loss is 3.706\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #93/1433: Loss is 3.662\n",
            "Accuracy for first subject is 31.25%, for second subject 40.62%\n",
            "Batch #94/1433: Loss is 3.665\n",
            "Accuracy for first subject is 37.50%, for second subject 40.62%\n",
            "Batch #95/1433: Loss is 3.826\n",
            "Accuracy for first subject is 28.12%, for second subject 25.00%\n",
            "Batch #96/1433: Loss is 3.581\n",
            "Accuracy for first subject is 37.50%, for second subject 43.75%\n",
            "Batch #97/1433: Loss is 3.561\n",
            "Accuracy for first subject is 37.50%, for second subject 50.00%\n",
            "Batch #98/1433: Loss is 3.763\n",
            "Accuracy for first subject is 37.50%, for second subject 21.88%\n",
            "Batch #99/1433: Loss is 3.718\n",
            "Accuracy for first subject is 37.50%, for second subject 37.50%\n",
            "Batch #100/1433: Loss is 3.810\n",
            "Accuracy for first subject is 31.25%, for second subject 34.38%\n",
            "Batch #101/1433: Loss is 3.679\n",
            "Accuracy for first subject is 40.62%, for second subject 34.38%\n",
            "Batch #102/1433: Loss is 3.645\n",
            "Accuracy for first subject is 40.62%, for second subject 37.50%\n",
            "Batch #103/1433: Loss is 3.947\n",
            "Accuracy for first subject is 15.62%, for second subject 25.00%\n",
            "Batch #104/1433: Loss is 3.658\n",
            "Accuracy for first subject is 37.50%, for second subject 40.62%\n",
            "Batch #105/1433: Loss is 3.860\n",
            "Accuracy for first subject is 25.00%, for second subject 28.12%\n",
            "Batch #106/1433: Loss is 3.652\n",
            "Accuracy for first subject is 40.62%, for second subject 31.25%\n",
            "Batch #107/1433: Loss is 3.794\n",
            "Accuracy for first subject is 34.38%, for second subject 28.12%\n",
            "Batch #108/1433: Loss is 3.714\n",
            "Accuracy for first subject is 40.62%, for second subject 37.50%\n",
            "Batch #109/1433: Loss is 3.758\n",
            "Accuracy for first subject is 21.88%, for second subject 37.50%\n",
            "Batch #110/1433: Loss is 3.752\n",
            "Accuracy for first subject is 43.75%, for second subject 25.00%\n",
            "Batch #111/1433: Loss is 3.775\n",
            "Accuracy for first subject is 21.88%, for second subject 28.12%\n",
            "Batch #112/1433: Loss is 3.610\n",
            "Accuracy for first subject is 50.00%, for second subject 25.00%\n",
            "Batch #113/1433: Loss is 3.615\n",
            "Accuracy for first subject is 46.88%, for second subject 34.38%\n",
            "Batch #114/1433: Loss is 3.790\n",
            "Accuracy for first subject is 21.88%, for second subject 31.25%\n",
            "Batch #115/1433: Loss is 3.743\n",
            "Accuracy for first subject is 31.25%, for second subject 31.25%\n",
            "Batch #116/1433: Loss is 3.705\n",
            "Accuracy for first subject is 34.38%, for second subject 37.50%\n",
            "Batch #117/1433: Loss is 3.747\n",
            "Accuracy for first subject is 28.12%, for second subject 21.88%\n",
            "Batch #118/1433: Loss is 3.580\n",
            "Accuracy for first subject is 50.00%, for second subject 53.12%\n",
            "Batch #119/1433: Loss is 3.708\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #120/1433: Loss is 3.657\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #121/1433: Loss is 3.734\n",
            "Accuracy for first subject is 25.00%, for second subject 40.62%\n",
            "Batch #122/1433: Loss is 3.581\n",
            "Accuracy for first subject is 50.00%, for second subject 50.00%\n",
            "Batch #123/1433: Loss is 3.654\n",
            "Accuracy for first subject is 40.62%, for second subject 34.38%\n",
            "Batch #124/1433: Loss is 3.617\n",
            "Accuracy for first subject is 37.50%, for second subject 46.88%\n",
            "Batch #125/1433: Loss is 3.693\n",
            "Accuracy for first subject is 37.50%, for second subject 37.50%\n",
            "Batch #126/1433: Loss is 3.723\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #127/1433: Loss is 3.948\n",
            "Accuracy for first subject is 21.88%, for second subject 15.62%\n",
            "Batch #128/1433: Loss is 3.586\n",
            "Accuracy for first subject is 28.12%, for second subject 46.88%\n",
            "Batch #129/1433: Loss is 3.791\n",
            "Accuracy for first subject is 34.38%, for second subject 25.00%\n",
            "Batch #130/1433: Loss is 3.721\n",
            "Accuracy for first subject is 21.88%, for second subject 40.62%\n",
            "Batch #131/1433: Loss is 3.523\n",
            "Accuracy for first subject is 37.50%, for second subject 56.25%\n",
            "Batch #132/1433: Loss is 3.719\n",
            "Accuracy for first subject is 34.38%, for second subject 34.38%\n",
            "Batch #133/1433: Loss is 3.602\n",
            "Accuracy for first subject is 34.38%, for second subject 50.00%\n",
            "Batch #134/1433: Loss is 3.803\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #135/1433: Loss is 3.734\n",
            "Accuracy for first subject is 18.75%, for second subject 37.50%\n",
            "Batch #136/1433: Loss is 3.762\n",
            "Accuracy for first subject is 31.25%, for second subject 21.88%\n",
            "Batch #137/1433: Loss is 3.667\n",
            "Accuracy for first subject is 31.25%, for second subject 43.75%\n",
            "Batch #138/1433: Loss is 3.673\n",
            "Accuracy for first subject is 28.12%, for second subject 31.25%\n",
            "Batch #139/1433: Loss is 3.611\n",
            "Accuracy for first subject is 46.88%, for second subject 37.50%\n",
            "Batch #140/1433: Loss is 3.713\n",
            "Accuracy for first subject is 43.75%, for second subject 21.88%\n",
            "Batch #141/1433: Loss is 3.653\n",
            "Accuracy for first subject is 28.12%, for second subject 37.50%\n",
            "Batch #142/1433: Loss is 3.660\n",
            "Accuracy for first subject is 50.00%, for second subject 28.12%\n",
            "Batch #143/1433: Loss is 3.722\n",
            "Accuracy for first subject is 25.00%, for second subject 34.38%\n",
            "Batch #144/1433: Loss is 3.755\n",
            "Accuracy for first subject is 40.62%, for second subject 37.50%\n",
            "Batch #145/1433: Loss is 3.871\n",
            "Accuracy for first subject is 28.12%, for second subject 21.88%\n",
            "Batch #146/1433: Loss is 3.790\n",
            "Accuracy for first subject is 21.88%, for second subject 34.38%\n",
            "Batch #147/1433: Loss is 3.829\n",
            "Accuracy for first subject is 31.25%, for second subject 21.88%\n",
            "Batch #148/1433: Loss is 3.687\n",
            "Accuracy for first subject is 28.12%, for second subject 40.62%\n",
            "Batch #149/1433: Loss is 3.577\n",
            "Accuracy for first subject is 28.12%, for second subject 50.00%\n",
            "Batch #150/1433: Loss is 3.590\n",
            "Accuracy for first subject is 37.50%, for second subject 43.75%\n",
            "Batch #151/1433: Loss is 3.698\n",
            "Accuracy for first subject is 31.25%, for second subject 40.62%\n",
            "Batch #152/1433: Loss is 3.677\n",
            "Accuracy for first subject is 28.12%, for second subject 40.62%\n",
            "Batch #153/1433: Loss is 3.848\n",
            "Accuracy for first subject is 15.62%, for second subject 25.00%\n",
            "Batch #154/1433: Loss is 3.778\n",
            "Accuracy for first subject is 31.25%, for second subject 34.38%\n",
            "Batch #155/1433: Loss is 3.811\n",
            "Accuracy for first subject is 15.62%, for second subject 28.12%\n",
            "Batch #156/1433: Loss is 3.601\n",
            "Accuracy for first subject is 40.62%, for second subject 40.62%\n",
            "Batch #157/1433: Loss is 3.620\n",
            "Accuracy for first subject is 43.75%, for second subject 40.62%\n",
            "Batch #158/1433: Loss is 3.743\n",
            "Accuracy for first subject is 28.12%, for second subject 31.25%\n",
            "Batch #159/1433: Loss is 3.693\n",
            "Accuracy for first subject is 46.88%, for second subject 31.25%\n",
            "Batch #160/1433: Loss is 3.643\n",
            "Accuracy for first subject is 31.25%, for second subject 43.75%\n",
            "Batch #161/1433: Loss is 3.673\n",
            "Accuracy for first subject is 46.88%, for second subject 31.25%\n",
            "Batch #162/1433: Loss is 3.597\n",
            "Accuracy for first subject is 43.75%, for second subject 40.62%\n",
            "Batch #163/1433: Loss is 3.654\n",
            "Accuracy for first subject is 50.00%, for second subject 28.12%\n",
            "Batch #164/1433: Loss is 3.572\n",
            "Accuracy for first subject is 37.50%, for second subject 50.00%\n",
            "Batch #165/1433: Loss is 3.716\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #166/1433: Loss is 3.669\n",
            "Accuracy for first subject is 43.75%, for second subject 37.50%\n",
            "Batch #167/1433: Loss is 3.717\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #168/1433: Loss is 3.707\n",
            "Accuracy for first subject is 34.38%, for second subject 43.75%\n",
            "Batch #169/1433: Loss is 3.739\n",
            "Accuracy for first subject is 37.50%, for second subject 34.38%\n",
            "Batch #170/1433: Loss is 3.624\n",
            "Accuracy for first subject is 43.75%, for second subject 40.62%\n",
            "Batch #171/1433: Loss is 3.720\n",
            "Accuracy for first subject is 28.12%, for second subject 40.62%\n",
            "Batch #172/1433: Loss is 3.793\n",
            "Accuracy for first subject is 31.25%, for second subject 31.25%\n",
            "Batch #173/1433: Loss is 3.679\n",
            "Accuracy for first subject is 40.62%, for second subject 21.88%\n",
            "Batch #174/1433: Loss is 3.559\n",
            "Accuracy for first subject is 46.88%, for second subject 40.62%\n",
            "Batch #175/1433: Loss is 3.595\n",
            "Accuracy for first subject is 43.75%, for second subject 31.25%\n",
            "Batch #176/1433: Loss is 3.615\n",
            "Accuracy for first subject is 40.62%, for second subject 40.62%\n",
            "Batch #177/1433: Loss is 3.682\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #178/1433: Loss is 3.736\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #179/1433: Loss is 3.768\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #180/1433: Loss is 3.683\n",
            "Accuracy for first subject is 34.38%, for second subject 34.38%\n",
            "Batch #181/1433: Loss is 3.656\n",
            "Accuracy for first subject is 40.62%, for second subject 37.50%\n",
            "Batch #182/1433: Loss is 3.731\n",
            "Accuracy for first subject is 28.12%, for second subject 37.50%\n",
            "Batch #183/1433: Loss is 3.693\n",
            "Accuracy for first subject is 34.38%, for second subject 37.50%\n",
            "Batch #184/1433: Loss is 3.752\n",
            "Accuracy for first subject is 25.00%, for second subject 37.50%\n",
            "Batch #185/1433: Loss is 3.665\n",
            "Accuracy for first subject is 37.50%, for second subject 46.88%\n",
            "Batch #186/1433: Loss is 3.683\n",
            "Accuracy for first subject is 25.00%, for second subject 40.62%\n",
            "Batch #187/1433: Loss is 3.785\n",
            "Accuracy for first subject is 15.62%, for second subject 31.25%\n",
            "Batch #188/1433: Loss is 3.664\n",
            "Accuracy for first subject is 43.75%, for second subject 40.62%\n",
            "Batch #189/1433: Loss is 3.528\n",
            "Accuracy for first subject is 46.88%, for second subject 53.12%\n",
            "Batch #190/1433: Loss is 3.552\n",
            "Accuracy for first subject is 59.38%, for second subject 37.50%\n",
            "Batch #191/1433: Loss is 3.744\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #192/1433: Loss is 3.754\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #193/1433: Loss is 3.728\n",
            "Accuracy for first subject is 34.38%, for second subject 21.88%\n",
            "Batch #194/1433: Loss is 3.649\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #195/1433: Loss is 3.808\n",
            "Accuracy for first subject is 21.88%, for second subject 31.25%\n",
            "Batch #196/1433: Loss is 3.832\n",
            "Accuracy for first subject is 25.00%, for second subject 15.62%\n",
            "Batch #197/1433: Loss is 3.746\n",
            "Accuracy for first subject is 34.38%, for second subject 21.88%\n",
            "Batch #198/1433: Loss is 3.692\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #199/1433: Loss is 3.661\n",
            "Accuracy for first subject is 37.50%, for second subject 43.75%\n",
            "Batch #200/1433: Loss is 3.711\n",
            "Accuracy for first subject is 25.00%, for second subject 31.25%\n",
            "Batch #201/1433: Loss is 3.734\n",
            "Accuracy for first subject is 25.00%, for second subject 21.88%\n",
            "Batch #202/1433: Loss is 3.787\n",
            "Accuracy for first subject is 15.62%, for second subject 15.62%\n",
            "Batch #203/1433: Loss is 3.715\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #204/1433: Loss is 3.687\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #205/1433: Loss is 3.640\n",
            "Accuracy for first subject is 43.75%, for second subject 43.75%\n",
            "Batch #206/1433: Loss is 3.633\n",
            "Accuracy for first subject is 46.88%, for second subject 34.38%\n",
            "Batch #207/1433: Loss is 3.595\n",
            "Accuracy for first subject is 50.00%, for second subject 43.75%\n",
            "Batch #208/1433: Loss is 3.726\n",
            "Accuracy for first subject is 34.38%, for second subject 25.00%\n",
            "Batch #209/1433: Loss is 3.687\n",
            "Accuracy for first subject is 40.62%, for second subject 34.38%\n",
            "Batch #210/1433: Loss is 3.716\n",
            "Accuracy for first subject is 31.25%, for second subject 25.00%\n",
            "Batch #211/1433: Loss is 3.679\n",
            "Accuracy for first subject is 28.12%, for second subject 40.62%\n",
            "Batch #212/1433: Loss is 3.557\n",
            "Accuracy for first subject is 46.88%, for second subject 46.88%\n",
            "Batch #213/1433: Loss is 3.641\n",
            "Accuracy for first subject is 34.38%, for second subject 34.38%\n",
            "Batch #214/1433: Loss is 3.643\n",
            "Accuracy for first subject is 31.25%, for second subject 40.62%\n",
            "Batch #215/1433: Loss is 3.532\n",
            "Accuracy for first subject is 34.38%, for second subject 46.88%\n",
            "Batch #216/1433: Loss is 3.767\n",
            "Accuracy for first subject is 25.00%, for second subject 28.12%\n",
            "Batch #217/1433: Loss is 3.833\n",
            "Accuracy for first subject is 25.00%, for second subject 18.75%\n",
            "Batch #218/1433: Loss is 3.612\n",
            "Accuracy for first subject is 31.25%, for second subject 46.88%\n",
            "Batch #219/1433: Loss is 3.638\n",
            "Accuracy for first subject is 25.00%, for second subject 43.75%\n",
            "Batch #220/1433: Loss is 3.697\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #221/1433: Loss is 3.596\n",
            "Accuracy for first subject is 43.75%, for second subject 46.88%\n",
            "Batch #222/1433: Loss is 3.814\n",
            "Accuracy for first subject is 34.38%, for second subject 18.75%\n",
            "Batch #223/1433: Loss is 3.758\n",
            "Accuracy for first subject is 34.38%, for second subject 28.12%\n",
            "Batch #224/1433: Loss is 3.819\n",
            "Accuracy for first subject is 15.62%, for second subject 21.88%\n",
            "Batch #225/1433: Loss is 3.743\n",
            "Accuracy for first subject is 25.00%, for second subject 25.00%\n",
            "Batch #226/1433: Loss is 3.641\n",
            "Accuracy for first subject is 37.50%, for second subject 40.62%\n",
            "Batch #227/1433: Loss is 3.691\n",
            "Accuracy for first subject is 34.38%, for second subject 40.62%\n",
            "Batch #228/1433: Loss is 3.731\n",
            "Accuracy for first subject is 25.00%, for second subject 28.12%\n",
            "Batch #229/1433: Loss is 3.691\n",
            "Accuracy for first subject is 31.25%, for second subject 34.38%\n",
            "Batch #230/1433: Loss is 3.793\n",
            "Accuracy for first subject is 18.75%, for second subject 18.75%\n",
            "Batch #231/1433: Loss is 3.636\n",
            "Accuracy for first subject is 34.38%, for second subject 40.62%\n",
            "Batch #232/1433: Loss is 3.694\n",
            "Accuracy for first subject is 34.38%, for second subject 34.38%\n",
            "Batch #233/1433: Loss is 3.654\n",
            "Accuracy for first subject is 37.50%, for second subject 34.38%\n",
            "Batch #234/1433: Loss is 3.580\n",
            "Accuracy for first subject is 50.00%, for second subject 40.62%\n",
            "Batch #235/1433: Loss is 3.616\n",
            "Accuracy for first subject is 40.62%, for second subject 37.50%\n",
            "Batch #236/1433: Loss is 3.613\n",
            "Accuracy for first subject is 37.50%, for second subject 43.75%\n",
            "Batch #237/1433: Loss is 3.761\n",
            "Accuracy for first subject is 21.88%, for second subject 31.25%\n",
            "Batch #238/1433: Loss is 3.723\n",
            "Accuracy for first subject is 25.00%, for second subject 31.25%\n",
            "Batch #239/1433: Loss is 3.678\n",
            "Accuracy for first subject is 46.88%, for second subject 31.25%\n",
            "Batch #240/1433: Loss is 3.734\n",
            "Accuracy for first subject is 34.38%, for second subject 21.88%\n",
            "Batch #241/1433: Loss is 3.721\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #242/1433: Loss is 3.724\n",
            "Accuracy for first subject is 40.62%, for second subject 28.12%\n",
            "Batch #243/1433: Loss is 3.667\n",
            "Accuracy for first subject is 40.62%, for second subject 37.50%\n",
            "Batch #244/1433: Loss is 3.645\n",
            "Accuracy for first subject is 43.75%, for second subject 37.50%\n",
            "Batch #245/1433: Loss is 3.661\n",
            "Accuracy for first subject is 40.62%, for second subject 40.62%\n",
            "Batch #246/1433: Loss is 3.728\n",
            "Accuracy for first subject is 28.12%, for second subject 37.50%\n",
            "Batch #247/1433: Loss is 3.719\n",
            "Accuracy for first subject is 37.50%, for second subject 21.88%\n",
            "Batch #248/1433: Loss is 3.738\n",
            "Accuracy for first subject is 37.50%, for second subject 21.88%\n",
            "Batch #249/1433: Loss is 3.600\n",
            "Accuracy for first subject is 46.88%, for second subject 37.50%\n",
            "Batch #250/1433: Loss is 3.746\n",
            "Accuracy for first subject is 28.12%, for second subject 15.62%\n",
            "Batch #251/1433: Loss is 3.721\n",
            "Accuracy for first subject is 21.88%, for second subject 34.38%\n",
            "Batch #252/1433: Loss is 3.636\n",
            "Accuracy for first subject is 43.75%, for second subject 40.62%\n",
            "Batch #253/1433: Loss is 3.702\n",
            "Accuracy for first subject is 34.38%, for second subject 25.00%\n",
            "Batch #254/1433: Loss is 3.681\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #255/1433: Loss is 3.697\n",
            "Accuracy for first subject is 25.00%, for second subject 40.62%\n",
            "Batch #256/1433: Loss is 3.703\n",
            "Accuracy for first subject is 28.12%, for second subject 31.25%\n",
            "Batch #257/1433: Loss is 3.780\n",
            "Accuracy for first subject is 25.00%, for second subject 21.88%\n",
            "Batch #258/1433: Loss is 3.656\n",
            "Accuracy for first subject is 43.75%, for second subject 31.25%\n",
            "Batch #259/1433: Loss is 3.636\n",
            "Accuracy for first subject is 34.38%, for second subject 34.38%\n",
            "Batch #260/1433: Loss is 3.641\n",
            "Accuracy for first subject is 40.62%, for second subject 34.38%\n",
            "Batch #261/1433: Loss is 3.630\n",
            "Accuracy for first subject is 34.38%, for second subject 40.62%\n",
            "Batch #262/1433: Loss is 3.673\n",
            "Accuracy for first subject is 40.62%, for second subject 28.12%\n",
            "Batch #263/1433: Loss is 3.739\n",
            "Accuracy for first subject is 43.75%, for second subject 21.88%\n",
            "Batch #264/1433: Loss is 3.688\n",
            "Accuracy for first subject is 28.12%, for second subject 25.00%\n",
            "Batch #265/1433: Loss is 3.764\n",
            "Accuracy for first subject is 18.75%, for second subject 31.25%\n",
            "Batch #266/1433: Loss is 3.747\n",
            "Accuracy for first subject is 25.00%, for second subject 31.25%\n",
            "Batch #267/1433: Loss is 3.704\n",
            "Accuracy for first subject is 28.12%, for second subject 37.50%\n",
            "Batch #268/1433: Loss is 3.677\n",
            "Accuracy for first subject is 40.62%, for second subject 34.38%\n",
            "Batch #269/1433: Loss is 3.716\n",
            "Accuracy for first subject is 34.38%, for second subject 28.12%\n",
            "Batch #270/1433: Loss is 3.703\n",
            "Accuracy for first subject is 28.12%, for second subject 21.88%\n",
            "Batch #271/1433: Loss is 3.596\n",
            "Accuracy for first subject is 37.50%, for second subject 46.88%\n",
            "Batch #272/1433: Loss is 3.633\n",
            "Accuracy for first subject is 46.88%, for second subject 31.25%\n",
            "Batch #273/1433: Loss is 3.724\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #274/1433: Loss is 3.531\n",
            "Accuracy for first subject is 43.75%, for second subject 50.00%\n",
            "Batch #275/1433: Loss is 3.701\n",
            "Accuracy for first subject is 21.88%, for second subject 31.25%\n",
            "Batch #276/1433: Loss is 3.635\n",
            "Accuracy for first subject is 40.62%, for second subject 43.75%\n",
            "Batch #277/1433: Loss is 3.671\n",
            "Accuracy for first subject is 37.50%, for second subject 43.75%\n",
            "Batch #278/1433: Loss is 3.753\n",
            "Accuracy for first subject is 28.12%, for second subject 31.25%\n",
            "Batch #279/1433: Loss is 3.774\n",
            "Accuracy for first subject is 21.88%, for second subject 28.12%\n",
            "Batch #280/1433: Loss is 3.720\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #281/1433: Loss is 3.624\n",
            "Accuracy for first subject is 40.62%, for second subject 40.62%\n",
            "Batch #282/1433: Loss is 3.668\n",
            "Accuracy for first subject is 43.75%, for second subject 34.38%\n",
            "Batch #283/1433: Loss is 3.663\n",
            "Accuracy for first subject is 37.50%, for second subject 34.38%\n",
            "Batch #284/1433: Loss is 3.706\n",
            "Accuracy for first subject is 31.25%, for second subject 31.25%\n",
            "Batch #285/1433: Loss is 3.715\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #286/1433: Loss is 3.667\n",
            "Accuracy for first subject is 40.62%, for second subject 40.62%\n",
            "Batch #287/1433: Loss is 3.718\n",
            "Accuracy for first subject is 40.62%, for second subject 31.25%\n",
            "Batch #288/1433: Loss is 3.750\n",
            "Accuracy for first subject is 21.88%, for second subject 21.88%\n",
            "Batch #289/1433: Loss is 3.614\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #290/1433: Loss is 3.680\n",
            "Accuracy for first subject is 28.12%, for second subject 37.50%\n",
            "Batch #291/1433: Loss is 3.681\n",
            "Accuracy for first subject is 37.50%, for second subject 34.38%\n",
            "Batch #292/1433: Loss is 3.728\n",
            "Accuracy for first subject is 21.88%, for second subject 40.62%\n",
            "Batch #293/1433: Loss is 3.531\n",
            "Accuracy for first subject is 46.88%, for second subject 37.50%\n",
            "Batch #294/1433: Loss is 3.401\n",
            "Accuracy for first subject is 59.38%, for second subject 46.88%\n",
            "Batch #295/1433: Loss is 3.703\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #296/1433: Loss is 3.669\n",
            "Accuracy for first subject is 37.50%, for second subject 40.62%\n",
            "Batch #297/1433: Loss is 3.636\n",
            "Accuracy for first subject is 34.38%, for second subject 28.12%\n",
            "Batch #298/1433: Loss is 3.599\n",
            "Accuracy for first subject is 50.00%, for second subject 50.00%\n",
            "Batch #299/1433: Loss is 3.651\n",
            "Accuracy for first subject is 40.62%, for second subject 31.25%\n",
            "Batch #300/1433: Loss is 3.693\n",
            "Accuracy for first subject is 34.38%, for second subject 25.00%\n",
            "Batch #301/1433: Loss is 3.719\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #302/1433: Loss is 3.820\n",
            "Accuracy for first subject is 25.00%, for second subject 28.12%\n",
            "Batch #303/1433: Loss is 3.692\n",
            "Accuracy for first subject is 37.50%, for second subject 34.38%\n",
            "Batch #304/1433: Loss is 3.581\n",
            "Accuracy for first subject is 50.00%, for second subject 34.38%\n",
            "Batch #305/1433: Loss is 3.685\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #306/1433: Loss is 3.605\n",
            "Accuracy for first subject is 43.75%, for second subject 43.75%\n",
            "Batch #307/1433: Loss is 3.592\n",
            "Accuracy for first subject is 46.88%, for second subject 40.62%\n",
            "Batch #308/1433: Loss is 3.673\n",
            "Accuracy for first subject is 37.50%, for second subject 34.38%\n",
            "Batch #309/1433: Loss is 3.685\n",
            "Accuracy for first subject is 37.50%, for second subject 25.00%\n",
            "Batch #310/1433: Loss is 3.741\n",
            "Accuracy for first subject is 31.25%, for second subject 25.00%\n",
            "Batch #311/1433: Loss is 3.641\n",
            "Accuracy for first subject is 43.75%, for second subject 31.25%\n",
            "Batch #312/1433: Loss is 3.606\n",
            "Accuracy for first subject is 50.00%, for second subject 25.00%\n",
            "Batch #313/1433: Loss is 3.668\n",
            "Accuracy for first subject is 40.62%, for second subject 37.50%\n",
            "Batch #314/1433: Loss is 3.788\n",
            "Accuracy for first subject is 21.88%, for second subject 25.00%\n",
            "Batch #315/1433: Loss is 3.761\n",
            "Accuracy for first subject is 18.75%, for second subject 43.75%\n",
            "Batch #316/1433: Loss is 3.623\n",
            "Accuracy for first subject is 37.50%, for second subject 40.62%\n",
            "Batch #317/1433: Loss is 3.691\n",
            "Accuracy for first subject is 34.38%, for second subject 37.50%\n",
            "Batch #318/1433: Loss is 3.627\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #319/1433: Loss is 3.693\n",
            "Accuracy for first subject is 31.25%, for second subject 40.62%\n",
            "Batch #320/1433: Loss is 3.658\n",
            "Accuracy for first subject is 40.62%, for second subject 31.25%\n",
            "Batch #321/1433: Loss is 3.586\n",
            "Accuracy for first subject is 37.50%, for second subject 43.75%\n",
            "Batch #322/1433: Loss is 3.656\n",
            "Accuracy for first subject is 31.25%, for second subject 34.38%\n",
            "Batch #323/1433: Loss is 3.666\n",
            "Accuracy for first subject is 37.50%, for second subject 31.25%\n",
            "Batch #324/1433: Loss is 3.658\n",
            "Accuracy for first subject is 21.88%, for second subject 34.38%\n",
            "Batch #325/1433: Loss is 3.713\n",
            "Accuracy for first subject is 28.12%, for second subject 25.00%\n",
            "Batch #326/1433: Loss is 3.755\n",
            "Accuracy for first subject is 25.00%, for second subject 21.88%\n",
            "Batch #327/1433: Loss is 3.626\n",
            "Accuracy for first subject is 34.38%, for second subject 40.62%\n",
            "Batch #328/1433: Loss is 3.667\n",
            "Accuracy for first subject is 31.25%, for second subject 21.88%\n",
            "Batch #329/1433: Loss is 3.721\n",
            "Accuracy for first subject is 25.00%, for second subject 37.50%\n",
            "Batch #330/1433: Loss is 3.643\n",
            "Accuracy for first subject is 50.00%, for second subject 18.75%\n",
            "Batch #331/1433: Loss is 3.757\n",
            "Accuracy for first subject is 25.00%, for second subject 21.88%\n",
            "Batch #332/1433: Loss is 3.750\n",
            "Accuracy for first subject is 21.88%, for second subject 25.00%\n",
            "Batch #333/1433: Loss is 3.714\n",
            "Accuracy for first subject is 37.50%, for second subject 18.75%\n",
            "Batch #334/1433: Loss is 3.663\n",
            "Accuracy for first subject is 37.50%, for second subject 50.00%\n",
            "Batch #335/1433: Loss is 3.592\n",
            "Accuracy for first subject is 40.62%, for second subject 40.62%\n",
            "Batch #336/1433: Loss is 3.798\n",
            "Accuracy for first subject is 18.75%, for second subject 15.62%\n",
            "Batch #337/1433: Loss is 3.584\n",
            "Accuracy for first subject is 46.88%, for second subject 43.75%\n",
            "Batch #338/1433: Loss is 3.667\n",
            "Accuracy for first subject is 31.25%, for second subject 37.50%\n",
            "Batch #339/1433: Loss is 3.560\n",
            "Accuracy for first subject is 46.88%, for second subject 40.62%\n",
            "Batch #340/1433: Loss is 3.496\n",
            "Accuracy for first subject is 40.62%, for second subject 50.00%\n",
            "Batch #341/1433: Loss is 3.777\n",
            "Accuracy for first subject is 25.00%, for second subject 21.88%\n",
            "Batch #342/1433: Loss is 3.696\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #343/1433: Loss is 3.669\n",
            "Accuracy for first subject is 50.00%, for second subject 43.75%\n",
            "Batch #344/1433: Loss is 3.646\n",
            "Accuracy for first subject is 34.38%, for second subject 25.00%\n",
            "Batch #345/1433: Loss is 3.635\n",
            "Accuracy for first subject is 37.50%, for second subject 34.38%\n",
            "Batch #346/1433: Loss is 3.727\n",
            "Accuracy for first subject is 21.88%, for second subject 34.38%\n",
            "Batch #347/1433: Loss is 3.722\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #348/1433: Loss is 3.670\n",
            "Accuracy for first subject is 37.50%, for second subject 25.00%\n",
            "Batch #349/1433: Loss is 3.695\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #350/1433: Loss is 3.629\n",
            "Accuracy for first subject is 28.12%, for second subject 31.25%\n",
            "Batch #351/1433: Loss is 3.648\n",
            "Accuracy for first subject is 40.62%, for second subject 46.88%\n",
            "Batch #352/1433: Loss is 3.607\n",
            "Accuracy for first subject is 46.88%, for second subject 43.75%\n",
            "Batch #353/1433: Loss is 3.687\n",
            "Accuracy for first subject is 31.25%, for second subject 31.25%\n",
            "Batch #354/1433: Loss is 3.629\n",
            "Accuracy for first subject is 40.62%, for second subject 40.62%\n",
            "Batch #355/1433: Loss is 3.589\n",
            "Accuracy for first subject is 46.88%, for second subject 40.62%\n",
            "Batch #356/1433: Loss is 3.695\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #357/1433: Loss is 3.752\n",
            "Accuracy for first subject is 18.75%, for second subject 34.38%\n",
            "Batch #358/1433: Loss is 3.715\n",
            "Accuracy for first subject is 21.88%, for second subject 40.62%\n",
            "Batch #359/1433: Loss is 3.747\n",
            "Accuracy for first subject is 18.75%, for second subject 28.12%\n",
            "Batch #360/1433: Loss is 3.736\n",
            "Accuracy for first subject is 46.88%, for second subject 31.25%\n",
            "Batch #361/1433: Loss is 3.636\n",
            "Accuracy for first subject is 34.38%, for second subject 50.00%\n",
            "Batch #362/1433: Loss is 3.544\n",
            "Accuracy for first subject is 31.25%, for second subject 40.62%\n",
            "Batch #363/1433: Loss is 3.609\n",
            "Accuracy for first subject is 46.88%, for second subject 43.75%\n",
            "Batch #364/1433: Loss is 3.696\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #365/1433: Loss is 3.694\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #366/1433: Loss is 3.652\n",
            "Accuracy for first subject is 37.50%, for second subject 28.12%\n",
            "Batch #367/1433: Loss is 3.613\n",
            "Accuracy for first subject is 37.50%, for second subject 46.88%\n",
            "Batch #368/1433: Loss is 3.812\n",
            "Accuracy for first subject is 15.62%, for second subject 9.38%\n",
            "Batch #369/1433: Loss is 3.705\n",
            "Accuracy for first subject is 21.88%, for second subject 37.50%\n",
            "Batch #370/1433: Loss is 3.645\n",
            "Accuracy for first subject is 31.25%, for second subject 34.38%\n",
            "Batch #371/1433: Loss is 3.745\n",
            "Accuracy for first subject is 25.00%, for second subject 37.50%\n",
            "Batch #372/1433: Loss is 3.619\n",
            "Accuracy for first subject is 37.50%, for second subject 40.62%\n",
            "Batch #373/1433: Loss is 3.634\n",
            "Accuracy for first subject is 37.50%, for second subject 34.38%\n",
            "Batch #374/1433: Loss is 3.617\n",
            "Accuracy for first subject is 37.50%, for second subject 43.75%\n",
            "Batch #375/1433: Loss is 3.589\n",
            "Accuracy for first subject is 46.88%, for second subject 34.38%\n",
            "Batch #376/1433: Loss is 3.598\n",
            "Accuracy for first subject is 43.75%, for second subject 34.38%\n",
            "Batch #377/1433: Loss is 3.780\n",
            "Accuracy for first subject is 28.12%, for second subject 15.62%\n",
            "Batch #378/1433: Loss is 3.595\n",
            "Accuracy for first subject is 40.62%, for second subject 46.88%\n",
            "Batch #379/1433: Loss is 3.649\n",
            "Accuracy for first subject is 40.62%, for second subject 34.38%\n",
            "Batch #380/1433: Loss is 3.626\n",
            "Accuracy for first subject is 34.38%, for second subject 43.75%\n",
            "Batch #381/1433: Loss is 3.592\n",
            "Accuracy for first subject is 40.62%, for second subject 28.12%\n",
            "Batch #382/1433: Loss is 3.722\n",
            "Accuracy for first subject is 34.38%, for second subject 21.88%\n",
            "Batch #383/1433: Loss is 3.528\n",
            "Accuracy for first subject is 50.00%, for second subject 34.38%\n",
            "Batch #384/1433: Loss is 3.743\n",
            "Accuracy for first subject is 21.88%, for second subject 28.12%\n",
            "Batch #385/1433: Loss is 3.623\n",
            "Accuracy for first subject is 28.12%, for second subject 46.88%\n",
            "Batch #386/1433: Loss is 3.627\n",
            "Accuracy for first subject is 34.38%, for second subject 31.25%\n",
            "Batch #387/1433: Loss is 3.710\n",
            "Accuracy for first subject is 31.25%, for second subject 31.25%\n",
            "Batch #388/1433: Loss is 3.689\n",
            "Accuracy for first subject is 25.00%, for second subject 28.12%\n",
            "Batch #389/1433: Loss is 3.658\n",
            "Accuracy for first subject is 28.12%, for second subject 37.50%\n",
            "Batch #390/1433: Loss is 3.519\n",
            "Accuracy for first subject is 46.88%, for second subject 56.25%\n",
            "Batch #391/1433: Loss is 3.763\n",
            "Accuracy for first subject is 21.88%, for second subject 28.12%\n",
            "Batch #392/1433: Loss is 3.577\n",
            "Accuracy for first subject is 46.88%, for second subject 37.50%\n",
            "Batch #393/1433: Loss is 3.663\n",
            "Accuracy for first subject is 28.12%, for second subject 34.38%\n",
            "Batch #394/1433: Loss is 3.756\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n",
            "Batch #395/1433: Loss is 3.671\n",
            "Accuracy for first subject is 37.50%, for second subject 28.12%\n",
            "Batch #396/1433: Loss is 3.694\n",
            "Accuracy for first subject is 31.25%, for second subject 46.88%\n",
            "Batch #397/1433: Loss is 3.724\n",
            "Accuracy for first subject is 25.00%, for second subject 43.75%\n",
            "Batch #398/1433: Loss is 3.598\n",
            "Accuracy for first subject is 40.62%, for second subject 50.00%\n",
            "Batch #399/1433: Loss is 3.681\n",
            "Accuracy for first subject is 40.62%, for second subject 31.25%\n",
            "Batch #400/1433: Loss is 3.724\n",
            "Accuracy for first subject is 28.12%, for second subject 28.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2uoPhU5cMUf"
      },
      "source": [
        "#losses = [loss.item() for loss in losses]\n",
        "#accuracies = [[accuracy[0].cpu().numpy(), accuracy[1].cpu().numpy()] for accuracy in accuracies]\n",
        "\n",
        "from matplotlib.pyplot import plot\n",
        "\n",
        "plt.xlabel('batch', fontsize=12)\n",
        "plt.ylabel('loss', fontsize=12)\n",
        "\n",
        "plot(list(range(len(losses))), losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US0s44g1yMt1"
      },
      "source": [
        "## saving logic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q04CcJcdGGB8"
      },
      "source": [
        "import secrets\n",
        "identifier_string = secrets.token_hex(nbytes=1)\n",
        "\n",
        "first_subject_model_name = f'subject_model_first_{identifier_string}_state.pth'\n",
        "second_subject_model_name = f'subject_model_second_{identifier_string}_state.pth'\n",
        "\n",
        "torch.save(first_subject_model.state_dict(), f\"/content/drive/MyDrive/Kool/Ãœlikool/3. aasta/LÃµputÃ¶Ã¶/{first_subject_model_name}\")\n",
        "torch.save(second_subject_model.state_dict(), f\"/content/drive/MyDrive/Kool/Ãœlikool/3. aasta/LÃµputÃ¶Ã¶/{second_subject_model_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}