{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Resnet + category model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnhyS1VLNrfh"
      },
      "source": [
        "# Starting out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAoN8e62MAif",
        "outputId": "2f923ca8-c98d-4d42-91b7-a1645ddabed3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wncr2153ROgV"
      },
      "source": [
        "!cp /content/drive/MyDrive/Kool/Ülikool/'3. aasta'/Lõputöö/data.zip .\n",
        "!unzip -q data.zip\n",
        "!rm data.zip\n",
        "\n",
        "!cp /content/drive/MyDrive/Kool/Ülikool/'3. aasta'/Lõputöö/test_val.zip .\n",
        "!unzip -q test_val.zip\n",
        "!rm test_val.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RgggMjkYEDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add8606f-4e0f-40bd-8a38-433f3e7a2ee6"
      },
      "source": [
        "!rm -rf line-chart-captioning/\n",
        "#clone repo\n",
        "!git clone https://github.com/snemvalts/line-chart-captioning\n",
        "#clean out data folder and move extracted raw data to captioning\n",
        "!rm -rf line-chart-captioning/data/*\n",
        "!mv data/** line-chart-captioning/data/\n",
        "!mv test_val/validation1 line-chart-captioning/data/figureqa/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'line-chart-captioning'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 129 (delta 40), reused 101 (delta 22), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (129/129), 22.30 KiB | 111.00 KiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjFeVDn2_pEo"
      },
      "source": [
        "import json\n",
        "\n",
        "# Smoothest and roughest seem to always appear. Same for MIN_AUC and MAX_AUC\n",
        "question_types = [\"GREATER\", \"LESS\", \"INTERSECT\"]\n",
        "synthetic_json = {\n",
        "  \"questions\": question_types\n",
        "}\n",
        "\n",
        "with open('line-chart-captioning/synthetic.json', 'w') as f:\n",
        "  json.dump(synthetic_json, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoZgEECwhGVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376917ef-4e91-43b0-92c8-809862175b7a"
      },
      "source": [
        "#VERY IMPORTANT THAT WE DON'T UNROLL\n",
        "!cd line-chart-captioning && \\\n",
        "python3 src/synthetic/preprocess-question-types.py \\\n",
        "--synthetic-config synthetic.json \\\n",
        "data/figureqa/train1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parsing QA...\n",
            "parsing annotations...\n",
            "processing plots...\n",
            "copying images...\n",
            "writing csv...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsMGUqAS8Ryw",
        "outputId": "34c83ba0-898c-4d0e-c857-08dd0dd915dd"
      },
      "source": [
        "#VERY IMPORTANT THAT WE DON'T UNROLL\n",
        "!cd line-chart-captioning && \\\n",
        "python3 src/synthetic/preprocess-question-types.py \\\n",
        "--synthetic-config synthetic.json \\\n",
        "data/figureqa/validation1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parsing QA...\n",
            "parsing annotations...\n",
            "processing plots...\n",
            "copying images...\n",
            "writing csv...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg4SToRdybsI"
      },
      "source": [
        "## creating transforms & dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQu5ntlOoscT"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from skimage import transform\n",
        "\n",
        "\n",
        "# adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "class SyntheticImageDataset(Dataset):\n",
        "  def __init__(self, csv_file, images_dir, transform=None):\n",
        "    self.charts_captions = pd.read_csv(csv_file)\n",
        "    self.images_dir = images_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.charts_captions)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    [image_number, description_blob, all_subjects_blob] = self.charts_captions.iloc[idx]\n",
        "    image_path = os.path.join(self.images_dir, f'{str(image_number)}.png')\n",
        "    image = np.array(Image.open(image_path))\n",
        "    \n",
        "    sample = {\n",
        "        'image': image, \n",
        "        'description_blob': description_blob,\n",
        "        'all_subjects_blob': all_subjects_blob   \n",
        "    }\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o2x0Jb8MDdM"
      },
      "source": [
        "class ResizeImage(object):\n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    resized_image = transform.resize(sample['image'], self.output_size)\n",
        "    return {**sample, 'image': resized_image}\n",
        "\n",
        "class StripImageTransparency(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    stripped_transparency_image = sample['image'][:,:,:3]\n",
        "    return {**sample, 'image': stripped_transparency_image}\n",
        "\n",
        "\n",
        "class NormalizeImage(object):\n",
        "  def __init__(self, mean, std):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    normalized_image = (sample['image'] - self.mean) / self.std\n",
        "    return {**sample, 'image': normalized_image}\n",
        "\n",
        "\n",
        "class ImageToTensor(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    image = sample['image'].transpose((2, 0, 1))\n",
        "    return {**sample, 'image': torch.tensor(image).float()}\n",
        "\n",
        "\n",
        "class QuestionTypesToOneHotTensor(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    descriptions = json.loads(sample['description_blob'])\n",
        "    descriptions_present = [description[0] for description in descriptions]\n",
        "    all_description_types = [0]*(len(question_types))\n",
        "\n",
        "    for present_description_type in descriptions_present:\n",
        "      description_type_index = question_types.index(present_description_type)\n",
        "      all_description_types[description_type_index] = 1\n",
        "\n",
        "    return {**sample, 'question_types': torch.tensor(all_description_types).float() }\n",
        "\n",
        "#TODO: how do we encapsulate subjects?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFYGum8CMi4h"
      },
      "source": [
        "import torch\n",
        "\n",
        "def get_dataset(images_dir = None, csv_file = None):\n",
        "  # parallely calculate max caption len and word map to pass it to padcaption\n",
        "  # cause can't access dataset from transforms before it is defined in compose\n",
        "\n",
        "  dataset = SyntheticImageDataset(images_dir = images_dir,\n",
        "                           csv_file = csv_file,\n",
        "                           transform=transforms.Compose([\n",
        "                                  ResizeImage((224, 224)),\n",
        "                                  StripImageTransparency(),\n",
        "                                  NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                                  ImageToTensor(),\n",
        "                                  QuestionTypesToOneHotTensor()\n",
        "                                 ]))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = get_dataset(images_dir = 'line-chart-captioning/data/processed_synthetic/train1-types/images',\n",
        "                      csv_file = 'line-chart-captioning/data/processed_synthetic/train1-types/captions.csv')\n",
        "\n",
        "\n",
        "validation_dataset = get_dataset(images_dir = 'line-chart-captioning/data/processed_synthetic/validation1-types/images',\n",
        "                      csv_file = 'line-chart-captioning/data/processed_synthetic/validation1-types/captions.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
        "\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "        validation_dataset,\n",
        "        batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VNbi6_FykrD"
      },
      "source": [
        "##  encoder and category model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCMmvlNbknM1"
      },
      "source": [
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, encoder_shape):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    base_resnet = torchvision.models.resnet50(pretrained=True) \n",
        "    resnet_without_fc = nn.Sequential(*(list(base_resnet.children())[:-2]))\n",
        "\n",
        "    # freeze weights of resnet \n",
        "    for parameter in resnet_without_fc.parameters():\n",
        "      parameter.requires_grad = False\n",
        "\n",
        "\n",
        "    self.resnet = resnet_without_fc\n",
        "    self.pool = nn.AdaptiveAvgPool3d(encoder_shape)\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.resnet(x)\n",
        "    x = x.squeeze()\n",
        "    x = self.pool(x)\n",
        "    x = torch.reshape(x, (-1, 2048*3*3))\n",
        "    return x\n",
        "\n",
        "class CategoryModel(nn.Module):\n",
        "  def __init__(self, encoder_shape, \n",
        "               hidden_dim, \n",
        "               category_count,\n",
        "               dropout_p=0.5):\n",
        "    super(CategoryModel, self).__init__()\n",
        "\n",
        "    encoder_dim = 1\n",
        "    for dim in encoder_shape:\n",
        "      encoder_dim *= dim\n",
        "\n",
        "\n",
        "    self.hidden1 = nn.Linear(encoder_dim, hidden_dim)\n",
        "    self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.hidden3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.hidden4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.hidden5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.hidden6 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "\n",
        "    self.output = nn.Linear(hidden_dim, category_count)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.softmax = nn.Softmax()\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    for m in [self.hidden1, self.hidden2, self.hidden3, self.hidden4, self.hidden5, self.hidden6]:\n",
        "      torch.nn.init.xavier_uniform(m.weight)\n",
        "      m.bias.data.fill_(0.01)\n",
        "\n",
        "  def forward(self, encoder_out):\n",
        "    x = self.relu(self.hidden1(encoder_out))\n",
        "    x = self.relu(self.hidden2(x))\n",
        "    x = self.relu(self.hidden3(x))\n",
        "\n",
        "    x = self.relu(self.hidden4(x))\n",
        "    x = self.relu(self.hidden5(x))\n",
        "    x = self.relu(self.hidden6(x))\n",
        "\n",
        "    x = self.output(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV-KG_i-es7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cff6af3b-87e5-4f09-8a59-029fe88d2980"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import random\n",
        "\n",
        "def accuracy(prediction_scores, target, pred_threshold=0.33):\n",
        "  predictions = torch.gt(prediction_scores, pred_threshold).int()\n",
        "  correct = torch.eq(predictions, target).sum()\n",
        "  correct = correct.sum()\n",
        "  total = target.shape[0] * target.shape[1]\n",
        "  return (correct/total)*100.0\n",
        "\n",
        "\n",
        "#Maybe needed\n",
        "#def calculate_loss(prediction_scores, targets, criterion, pred_threshold=0.5):\n",
        "#  losses = []\n",
        "#  for i, prediction in enumerate(prediction_scores):\n",
        "#    true_prediction_indices = ((prediction >= pred_threshold).nonzero(as_tuple=True))\n",
        "#    for index in true_prediction_indices:\n",
        "#      losses.append(criterion(prediction[index], targets[i]))\n",
        "  \n",
        "#  return sum(losses)\n",
        "\n",
        "\n",
        "def train_epoch(iter_cap=50):\n",
        "  global min_loss\n",
        "\n",
        "  iter_without_improvement = 0\n",
        "\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    imgs = batch['image'].to(device)\n",
        "    target_types = batch['question_types'].to(device)\n",
        "\n",
        "    imgs = encoder(imgs)\n",
        "    prediction_scores = category_model(imgs)\n",
        "\n",
        "    #print(prediction_scores.shape, target_types.shape)\n",
        "    # Calculate loss\n",
        "    loss = criterion(prediction_scores, target_types)\n",
        "\n",
        "    category_model_optimizer.zero_grad()\n",
        "    #encoder_optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    category_model_optimizer.step()\n",
        "    #encoder_optimizer.step()\n",
        "\n",
        "    loss_score = loss.cpu().detach().numpy()\n",
        "    if (loss_score <= min_loss):\n",
        "      min_loss = loss_score\n",
        "      iter_without_improvement = 0\n",
        "    else:\n",
        "      iter_without_improvement += 1\n",
        "\n",
        "    accuracy_score = accuracy(prediction_scores, target_types, pred_threshold=pred_threshold)\n",
        "\n",
        "    print(f\"Batch #{i}/{len(train_loader)}: Loss is {loss_score:.3f}, Accuracy is {accuracy_score:.1f}%\")\n",
        "    if (i % 15 == 0):\n",
        "      validate(next(iter(validation_loader)))\n",
        "\n",
        "    if (iter_without_improvement >= iter_cap):\n",
        "      print(f\"{iter_cap} iterations without improvement. stopping\")\n",
        "      break\n",
        "\n",
        "\n",
        "def validate(batch):\n",
        "  global total_count\n",
        "  global predicted_present_count\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imgs = batch['image'].to(device)\n",
        "    target_types = batch['question_types'].to(device)\n",
        "\n",
        "    imgs = encoder(imgs)\n",
        "\n",
        "    category_prediction_scores = category_model(imgs)\n",
        "    accuracy_score = accuracy(category_prediction_scores, target_types, pred_threshold=pred_threshold)\n",
        "    print(f\"Validation accuracy: {accuracy_score:.1f}%\")\n",
        "\n",
        "\n",
        "encoder_shape = (2048, 3, 3)\n",
        "hidden_dim = 2048\n",
        "category_count = len(question_types)\n",
        "pred_threshold = 1/category_count\n",
        "\n",
        "category_model_lr = 3e-5\n",
        "encoder_lr = 3e-4\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = Encoder(encoder_shape=encoder_shape).to(device)\n",
        "#encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "#                                             lr=encoder_lr)\n",
        "\n",
        "\n",
        "category_model = CategoryModel(encoder_shape, hidden_dim, category_count).to(device)\n",
        "category_model_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, category_model.parameters()),\n",
        "                                             lr=category_model_lr)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "min_loss = float('inf')\n",
        "iter_without_improvement = 0\n",
        "\n",
        "category_model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch + 1} of {epochs}\")\n",
        "  train_epoch(iter_cap = 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 of 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch #0/717: Loss is 0.762, Accuracy is 48.4%\n",
            "Validation accuracy: 46.9%\n",
            "Batch #1/717: Loss is 0.761, Accuracy is 53.1%\n",
            "Batch #2/717: Loss is 0.765, Accuracy is 46.4%\n",
            "Batch #3/717: Loss is 0.762, Accuracy is 47.9%\n",
            "Batch #4/717: Loss is 0.765, Accuracy is 45.8%\n",
            "Batch #5/717: Loss is 0.764, Accuracy is 44.8%\n",
            "Batch #6/717: Loss is 0.765, Accuracy is 42.7%\n",
            "Batch #7/717: Loss is 0.756, Accuracy is 47.9%\n",
            "Batch #8/717: Loss is 0.768, Accuracy is 47.4%\n",
            "Batch #9/717: Loss is 0.762, Accuracy is 52.6%\n",
            "Batch #10/717: Loss is 0.758, Accuracy is 55.7%\n",
            "Batch #11/717: Loss is 0.761, Accuracy is 53.1%\n",
            "Batch #12/717: Loss is 0.764, Accuracy is 50.5%\n",
            "Batch #13/717: Loss is 0.764, Accuracy is 46.9%\n",
            "Batch #14/717: Loss is 0.761, Accuracy is 52.6%\n",
            "Batch #15/717: Loss is 0.763, Accuracy is 49.0%\n",
            "Validation accuracy: 45.3%\n",
            "Batch #16/717: Loss is 0.760, Accuracy is 46.4%\n",
            "Batch #17/717: Loss is 0.763, Accuracy is 48.4%\n",
            "Batch #18/717: Loss is 0.756, Accuracy is 50.0%\n",
            "Batch #19/717: Loss is 0.759, Accuracy is 46.9%\n",
            "Batch #20/717: Loss is 0.763, Accuracy is 45.8%\n",
            "Batch #21/717: Loss is 0.759, Accuracy is 48.4%\n",
            "Batch #22/717: Loss is 0.756, Accuracy is 58.3%\n",
            "Batch #23/717: Loss is 0.762, Accuracy is 51.6%\n",
            "Batch #24/717: Loss is 0.773, Accuracy is 49.0%\n",
            "Batch #25/717: Loss is 0.761, Accuracy is 50.0%\n",
            "Batch #26/717: Loss is 0.755, Accuracy is 49.5%\n",
            "Batch #27/717: Loss is 0.760, Accuracy is 47.9%\n",
            "Batch #28/717: Loss is 0.770, Accuracy is 43.8%\n",
            "Batch #29/717: Loss is 0.762, Accuracy is 45.3%\n",
            "Batch #30/717: Loss is 0.758, Accuracy is 51.0%\n",
            "Validation accuracy: 49.5%\n",
            "Batch #31/717: Loss is 0.747, Accuracy is 55.7%\n",
            "Batch #32/717: Loss is 0.768, Accuracy is 52.1%\n",
            "Batch #33/717: Loss is 0.760, Accuracy is 61.5%\n",
            "Batch #34/717: Loss is 0.743, Accuracy is 65.1%\n",
            "Batch #35/717: Loss is 0.774, Accuracy is 54.2%\n",
            "Batch #36/717: Loss is 0.773, Accuracy is 55.7%\n",
            "Batch #37/717: Loss is 0.765, Accuracy is 54.2%\n",
            "Batch #38/717: Loss is 0.768, Accuracy is 56.2%\n",
            "Batch #39/717: Loss is 0.772, Accuracy is 51.0%\n",
            "Batch #40/717: Loss is 0.752, Accuracy is 63.0%\n",
            "Batch #41/717: Loss is 0.772, Accuracy is 52.1%\n",
            "Batch #42/717: Loss is 0.758, Accuracy is 61.5%\n",
            "Batch #43/717: Loss is 0.762, Accuracy is 53.1%\n",
            "Batch #44/717: Loss is 0.766, Accuracy is 52.6%\n",
            "Batch #45/717: Loss is 0.759, Accuracy is 55.7%\n",
            "Validation accuracy: 52.6%\n",
            "Batch #46/717: Loss is 0.766, Accuracy is 47.4%\n",
            "Batch #47/717: Loss is 0.768, Accuracy is 46.4%\n",
            "Batch #48/717: Loss is 0.755, Accuracy is 49.0%\n",
            "Batch #49/717: Loss is 0.759, Accuracy is 50.5%\n",
            "Batch #50/717: Loss is 0.760, Accuracy is 48.4%\n",
            "Batch #51/717: Loss is 0.758, Accuracy is 52.1%\n",
            "Batch #52/717: Loss is 0.763, Accuracy is 46.4%\n",
            "Batch #53/717: Loss is 0.757, Accuracy is 56.2%\n",
            "Batch #54/717: Loss is 0.764, Accuracy is 48.4%\n",
            "Batch #55/717: Loss is 0.760, Accuracy is 53.1%\n",
            "Batch #56/717: Loss is 0.753, Accuracy is 56.8%\n",
            "Batch #57/717: Loss is 0.747, Accuracy is 58.3%\n",
            "Batch #58/717: Loss is 0.753, Accuracy is 54.2%\n",
            "Batch #59/717: Loss is 0.756, Accuracy is 54.2%\n",
            "Batch #60/717: Loss is 0.759, Accuracy is 53.6%\n",
            "Validation accuracy: 56.2%\n",
            "Batch #61/717: Loss is 0.762, Accuracy is 53.1%\n",
            "Batch #62/717: Loss is 0.761, Accuracy is 49.5%\n",
            "Batch #63/717: Loss is 0.757, Accuracy is 55.2%\n",
            "Batch #64/717: Loss is 0.766, Accuracy is 50.0%\n",
            "Batch #65/717: Loss is 0.745, Accuracy is 56.2%\n",
            "Batch #66/717: Loss is 0.759, Accuracy is 51.0%\n",
            "Batch #67/717: Loss is 0.761, Accuracy is 50.5%\n",
            "Batch #68/717: Loss is 0.756, Accuracy is 55.7%\n",
            "Batch #69/717: Loss is 0.757, Accuracy is 54.2%\n",
            "Batch #70/717: Loss is 0.765, Accuracy is 53.6%\n",
            "Batch #71/717: Loss is 0.757, Accuracy is 59.4%\n",
            "Batch #72/717: Loss is 0.750, Accuracy is 60.4%\n",
            "Batch #73/717: Loss is 0.764, Accuracy is 51.0%\n",
            "Batch #74/717: Loss is 0.757, Accuracy is 55.2%\n",
            "Batch #75/717: Loss is 0.744, Accuracy is 57.8%\n",
            "Validation accuracy: 48.4%\n",
            "Batch #76/717: Loss is 0.769, Accuracy is 48.4%\n",
            "Batch #77/717: Loss is 0.756, Accuracy is 56.2%\n",
            "Batch #78/717: Loss is 0.763, Accuracy is 49.5%\n",
            "Batch #79/717: Loss is 0.765, Accuracy is 53.6%\n",
            "Batch #80/717: Loss is 0.768, Accuracy is 54.2%\n",
            "Batch #81/717: Loss is 0.775, Accuracy is 46.9%\n",
            "Batch #82/717: Loss is 0.748, Accuracy is 58.3%\n",
            "Batch #83/717: Loss is 0.774, Accuracy is 46.9%\n",
            "Batch #84/717: Loss is 0.761, Accuracy is 49.5%\n",
            "Batch #85/717: Loss is 0.755, Accuracy is 52.6%\n",
            "Batch #86/717: Loss is 0.755, Accuracy is 54.2%\n",
            "Batch #87/717: Loss is 0.765, Accuracy is 46.4%\n",
            "Batch #88/717: Loss is 0.756, Accuracy is 50.5%\n",
            "Batch #89/717: Loss is 0.755, Accuracy is 54.7%\n",
            "Batch #90/717: Loss is 0.761, Accuracy is 50.0%\n",
            "Validation accuracy: 47.9%\n",
            "Batch #91/717: Loss is 0.755, Accuracy is 48.4%\n",
            "Batch #92/717: Loss is 0.765, Accuracy is 45.8%\n",
            "Batch #93/717: Loss is 0.761, Accuracy is 47.4%\n",
            "Batch #94/717: Loss is 0.756, Accuracy is 53.6%\n",
            "Batch #95/717: Loss is 0.763, Accuracy is 48.4%\n",
            "Batch #96/717: Loss is 0.759, Accuracy is 46.4%\n",
            "Batch #97/717: Loss is 0.758, Accuracy is 51.0%\n",
            "Batch #98/717: Loss is 0.757, Accuracy is 50.0%\n",
            "Batch #99/717: Loss is 0.771, Accuracy is 51.0%\n",
            "Batch #100/717: Loss is 0.764, Accuracy is 57.8%\n",
            "Batch #101/717: Loss is 0.765, Accuracy is 52.1%\n",
            "Batch #102/717: Loss is 0.769, Accuracy is 59.4%\n",
            "Batch #103/717: Loss is 0.770, Accuracy is 52.1%\n",
            "Batch #104/717: Loss is 0.765, Accuracy is 55.2%\n",
            "Batch #105/717: Loss is 0.760, Accuracy is 54.2%\n",
            "Validation accuracy: 53.1%\n",
            "Batch #106/717: Loss is 0.770, Accuracy is 49.0%\n",
            "Batch #107/717: Loss is 0.762, Accuracy is 50.5%\n",
            "Batch #108/717: Loss is 0.759, Accuracy is 53.6%\n",
            "Batch #109/717: Loss is 0.764, Accuracy is 48.4%\n",
            "Batch #110/717: Loss is 0.764, Accuracy is 49.0%\n",
            "Batch #111/717: Loss is 0.759, Accuracy is 51.0%\n",
            "Batch #112/717: Loss is 0.755, Accuracy is 51.6%\n",
            "Batch #113/717: Loss is 0.753, Accuracy is 54.2%\n",
            "Batch #114/717: Loss is 0.757, Accuracy is 47.4%\n",
            "Batch #115/717: Loss is 0.755, Accuracy is 55.7%\n",
            "Batch #116/717: Loss is 0.752, Accuracy is 55.2%\n",
            "Batch #117/717: Loss is 0.758, Accuracy is 56.2%\n",
            "Batch #118/717: Loss is 0.761, Accuracy is 53.1%\n",
            "Batch #119/717: Loss is 0.739, Accuracy is 64.1%\n",
            "Batch #120/717: Loss is 0.759, Accuracy is 55.7%\n",
            "Validation accuracy: 62.5%\n",
            "Batch #121/717: Loss is 0.769, Accuracy is 52.6%\n",
            "Batch #122/717: Loss is 0.757, Accuracy is 55.7%\n",
            "Batch #123/717: Loss is 0.758, Accuracy is 55.2%\n",
            "Batch #124/717: Loss is 0.748, Accuracy is 60.4%\n",
            "Batch #125/717: Loss is 0.757, Accuracy is 50.5%\n",
            "Batch #126/717: Loss is 0.759, Accuracy is 54.2%\n",
            "Batch #127/717: Loss is 0.754, Accuracy is 57.3%\n",
            "Batch #128/717: Loss is 0.747, Accuracy is 56.8%\n",
            "Batch #129/717: Loss is 0.770, Accuracy is 51.6%\n",
            "Batch #130/717: Loss is 0.756, Accuracy is 54.2%\n",
            "Batch #131/717: Loss is 0.753, Accuracy is 57.3%\n",
            "Batch #132/717: Loss is 0.743, Accuracy is 54.7%\n",
            "Batch #133/717: Loss is 0.747, Accuracy is 60.4%\n",
            "Batch #134/717: Loss is 0.755, Accuracy is 49.5%\n",
            "Batch #135/717: Loss is 0.741, Accuracy is 61.5%\n",
            "Validation accuracy: 49.5%\n",
            "Batch #136/717: Loss is 0.768, Accuracy is 47.9%\n",
            "Batch #137/717: Loss is 0.743, Accuracy is 59.4%\n",
            "Batch #138/717: Loss is 0.767, Accuracy is 51.6%\n",
            "Batch #139/717: Loss is 0.783, Accuracy is 44.3%\n",
            "Batch #140/717: Loss is 0.761, Accuracy is 47.4%\n",
            "Batch #141/717: Loss is 0.758, Accuracy is 49.0%\n",
            "Batch #142/717: Loss is 0.757, Accuracy is 52.1%\n",
            "Batch #143/717: Loss is 0.758, Accuracy is 48.4%\n",
            "Batch #144/717: Loss is 0.754, Accuracy is 52.1%\n",
            "Batch #145/717: Loss is 0.753, Accuracy is 52.1%\n",
            "Batch #146/717: Loss is 0.749, Accuracy is 52.1%\n",
            "Batch #147/717: Loss is 0.744, Accuracy is 56.2%\n",
            "Batch #148/717: Loss is 0.767, Accuracy is 52.1%\n",
            "Batch #149/717: Loss is 0.765, Accuracy is 51.6%\n",
            "Batch #150/717: Loss is 0.756, Accuracy is 56.8%\n",
            "Validation accuracy: 57.3%\n",
            "Batch #151/717: Loss is 0.749, Accuracy is 63.0%\n",
            "Batch #152/717: Loss is 0.752, Accuracy is 60.4%\n",
            "Batch #153/717: Loss is 0.760, Accuracy is 58.9%\n",
            "Batch #154/717: Loss is 0.759, Accuracy is 55.7%\n",
            "Batch #155/717: Loss is 0.755, Accuracy is 59.4%\n",
            "Batch #156/717: Loss is 0.781, Accuracy is 47.4%\n",
            "Batch #157/717: Loss is 0.755, Accuracy is 55.2%\n",
            "Batch #158/717: Loss is 0.754, Accuracy is 55.2%\n",
            "Batch #159/717: Loss is 0.757, Accuracy is 53.1%\n",
            "Batch #160/717: Loss is 0.756, Accuracy is 54.2%\n",
            "Batch #161/717: Loss is 0.755, Accuracy is 52.6%\n",
            "Batch #162/717: Loss is 0.757, Accuracy is 48.4%\n",
            "Batch #163/717: Loss is 0.762, Accuracy is 49.5%\n",
            "Batch #164/717: Loss is 0.762, Accuracy is 47.4%\n",
            "Batch #165/717: Loss is 0.755, Accuracy is 52.1%\n",
            "Validation accuracy: 50.0%\n",
            "Batch #166/717: Loss is 0.749, Accuracy is 55.2%\n",
            "Batch #167/717: Loss is 0.763, Accuracy is 49.0%\n",
            "Batch #168/717: Loss is 0.768, Accuracy is 46.9%\n",
            "Batch #169/717: Loss is 0.768, Accuracy is 47.9%\n",
            "Batch #170/717: Loss is 0.763, Accuracy is 51.6%\n",
            "Batch #171/717: Loss is 0.770, Accuracy is 43.2%\n",
            "Batch #172/717: Loss is 0.772, Accuracy is 43.8%\n",
            "Batch #173/717: Loss is 0.762, Accuracy is 48.4%\n",
            "Batch #174/717: Loss is 0.757, Accuracy is 48.4%\n",
            "Batch #175/717: Loss is 0.758, Accuracy is 49.5%\n",
            "Batch #176/717: Loss is 0.750, Accuracy is 50.0%\n",
            "Batch #177/717: Loss is 0.749, Accuracy is 55.7%\n",
            "Batch #178/717: Loss is 0.754, Accuracy is 48.4%\n",
            "Batch #179/717: Loss is 0.770, Accuracy is 45.3%\n",
            "Batch #180/717: Loss is 0.749, Accuracy is 54.2%\n",
            "Validation accuracy: 43.8%\n",
            "Batch #181/717: Loss is 0.756, Accuracy is 49.0%\n",
            "Batch #182/717: Loss is 0.763, Accuracy is 50.0%\n",
            "Batch #183/717: Loss is 0.761, Accuracy is 50.5%\n",
            "Batch #184/717: Loss is 0.758, Accuracy is 52.6%\n",
            "Batch #185/717: Loss is 0.767, Accuracy is 52.1%\n",
            "Batch #186/717: Loss is 0.766, Accuracy is 51.6%\n",
            "Batch #187/717: Loss is 0.754, Accuracy is 54.2%\n",
            "Batch #188/717: Loss is 0.760, Accuracy is 52.6%\n",
            "Batch #189/717: Loss is 0.770, Accuracy is 50.5%\n",
            "Batch #190/717: Loss is 0.742, Accuracy is 61.5%\n",
            "Batch #191/717: Loss is 0.764, Accuracy is 50.5%\n",
            "Batch #192/717: Loss is 0.761, Accuracy is 51.6%\n",
            "Batch #193/717: Loss is 0.750, Accuracy is 56.8%\n",
            "Batch #194/717: Loss is 0.752, Accuracy is 54.7%\n",
            "Batch #195/717: Loss is 0.754, Accuracy is 50.5%\n",
            "Validation accuracy: 50.5%\n",
            "Batch #196/717: Loss is 0.772, Accuracy is 41.7%\n",
            "Batch #197/717: Loss is 0.761, Accuracy is 49.0%\n",
            "Batch #198/717: Loss is 0.761, Accuracy is 47.4%\n",
            "Batch #199/717: Loss is 0.766, Accuracy is 46.9%\n",
            "Batch #200/717: Loss is 0.763, Accuracy is 46.4%\n",
            "Batch #201/717: Loss is 0.748, Accuracy is 55.2%\n",
            "Batch #202/717: Loss is 0.759, Accuracy is 47.9%\n",
            "Batch #203/717: Loss is 0.756, Accuracy is 50.5%\n",
            "Batch #204/717: Loss is 0.761, Accuracy is 45.8%\n",
            "Batch #205/717: Loss is 0.755, Accuracy is 49.5%\n",
            "Batch #206/717: Loss is 0.748, Accuracy is 55.7%\n",
            "Batch #207/717: Loss is 0.756, Accuracy is 49.0%\n",
            "Batch #208/717: Loss is 0.757, Accuracy is 51.0%\n",
            "Batch #209/717: Loss is 0.758, Accuracy is 52.6%\n",
            "Batch #210/717: Loss is 0.764, Accuracy is 49.5%\n",
            "Validation accuracy: 50.5%\n",
            "Batch #211/717: Loss is 0.744, Accuracy is 54.7%\n",
            "Batch #212/717: Loss is 0.761, Accuracy is 49.5%\n",
            "Batch #213/717: Loss is 0.753, Accuracy is 54.2%\n",
            "Batch #214/717: Loss is 0.747, Accuracy is 56.2%\n",
            "Batch #215/717: Loss is 0.765, Accuracy is 48.4%\n",
            "Batch #216/717: Loss is 0.767, Accuracy is 45.8%\n",
            "Batch #217/717: Loss is 0.745, Accuracy is 57.8%\n",
            "Batch #218/717: Loss is 0.760, Accuracy is 51.6%\n",
            "Batch #219/717: Loss is 0.757, Accuracy is 52.6%\n",
            "Batch #220/717: Loss is 0.765, Accuracy is 46.9%\n",
            "Batch #221/717: Loss is 0.767, Accuracy is 45.3%\n",
            "Batch #222/717: Loss is 0.774, Accuracy is 43.2%\n",
            "Batch #223/717: Loss is 0.763, Accuracy is 48.4%\n",
            "Batch #224/717: Loss is 0.756, Accuracy is 51.6%\n",
            "Batch #225/717: Loss is 0.750, Accuracy is 55.2%\n",
            "Validation accuracy: 40.1%\n",
            "Batch #226/717: Loss is 0.756, Accuracy is 50.0%\n",
            "Batch #227/717: Loss is 0.769, Accuracy is 44.8%\n",
            "Batch #228/717: Loss is 0.750, Accuracy is 53.6%\n",
            "Batch #229/717: Loss is 0.751, Accuracy is 52.6%\n",
            "Batch #230/717: Loss is 0.752, Accuracy is 52.1%\n",
            "Batch #231/717: Loss is 0.756, Accuracy is 49.5%\n",
            "Batch #232/717: Loss is 0.763, Accuracy is 46.9%\n",
            "Batch #233/717: Loss is 0.750, Accuracy is 54.7%\n",
            "Batch #234/717: Loss is 0.752, Accuracy is 54.2%\n",
            "Batch #235/717: Loss is 0.766, Accuracy is 46.4%\n",
            "Batch #236/717: Loss is 0.759, Accuracy is 49.5%\n",
            "Batch #237/717: Loss is 0.757, Accuracy is 51.0%\n",
            "Batch #238/717: Loss is 0.751, Accuracy is 53.1%\n",
            "Batch #239/717: Loss is 0.743, Accuracy is 58.3%\n",
            "Batch #240/717: Loss is 0.762, Accuracy is 45.3%\n",
            "Validation accuracy: 47.4%\n",
            "Batch #241/717: Loss is 0.746, Accuracy is 54.7%\n",
            "Batch #242/717: Loss is 0.746, Accuracy is 53.1%\n",
            "Batch #243/717: Loss is 0.764, Accuracy is 47.4%\n",
            "Batch #244/717: Loss is 0.750, Accuracy is 54.2%\n",
            "Batch #245/717: Loss is 0.735, Accuracy is 58.3%\n",
            "Batch #246/717: Loss is 0.738, Accuracy is 57.8%\n",
            "Batch #247/717: Loss is 0.734, Accuracy is 56.8%\n",
            "Batch #248/717: Loss is 0.753, Accuracy is 55.7%\n",
            "Batch #249/717: Loss is 0.749, Accuracy is 53.1%\n",
            "Batch #250/717: Loss is 0.765, Accuracy is 51.0%\n",
            "Batch #251/717: Loss is 0.757, Accuracy is 51.0%\n",
            "Batch #252/717: Loss is 0.750, Accuracy is 53.6%\n",
            "Batch #253/717: Loss is 0.733, Accuracy is 59.9%\n",
            "Batch #254/717: Loss is 0.749, Accuracy is 52.6%\n",
            "Batch #255/717: Loss is 0.763, Accuracy is 47.9%\n",
            "Validation accuracy: 51.0%\n",
            "Batch #256/717: Loss is 0.766, Accuracy is 41.7%\n",
            "Batch #257/717: Loss is 0.742, Accuracy is 51.6%\n",
            "Batch #258/717: Loss is 0.755, Accuracy is 51.6%\n",
            "Batch #259/717: Loss is 0.758, Accuracy is 50.5%\n",
            "Batch #260/717: Loss is 0.768, Accuracy is 46.4%\n",
            "Batch #261/717: Loss is 0.759, Accuracy is 52.1%\n",
            "Batch #262/717: Loss is 0.765, Accuracy is 46.9%\n",
            "Batch #263/717: Loss is 0.762, Accuracy is 45.8%\n",
            "Batch #264/717: Loss is 0.753, Accuracy is 54.2%\n",
            "Batch #265/717: Loss is 0.746, Accuracy is 57.8%\n",
            "Batch #266/717: Loss is 0.763, Accuracy is 53.1%\n",
            "Batch #267/717: Loss is 0.749, Accuracy is 57.3%\n",
            "Batch #268/717: Loss is 0.748, Accuracy is 54.2%\n",
            "Batch #269/717: Loss is 0.757, Accuracy is 55.7%\n",
            "Batch #270/717: Loss is 0.766, Accuracy is 50.5%\n",
            "Validation accuracy: 61.5%\n",
            "Batch #271/717: Loss is 0.757, Accuracy is 52.1%\n",
            "Batch #272/717: Loss is 0.753, Accuracy is 57.3%\n",
            "Batch #273/717: Loss is 0.755, Accuracy is 55.2%\n",
            "Batch #274/717: Loss is 0.742, Accuracy is 51.0%\n",
            "Batch #275/717: Loss is 0.775, Accuracy is 47.4%\n",
            "Batch #276/717: Loss is 0.755, Accuracy is 50.5%\n",
            "Batch #277/717: Loss is 0.757, Accuracy is 52.1%\n",
            "Batch #278/717: Loss is 0.757, Accuracy is 50.5%\n",
            "Batch #279/717: Loss is 0.748, Accuracy is 55.2%\n",
            "Batch #280/717: Loss is 0.772, Accuracy is 42.7%\n",
            "Batch #281/717: Loss is 0.745, Accuracy is 58.9%\n",
            "Batch #282/717: Loss is 0.760, Accuracy is 46.9%\n",
            "Batch #283/717: Loss is 0.758, Accuracy is 51.0%\n",
            "Batch #284/717: Loss is 0.765, Accuracy is 49.0%\n",
            "Batch #285/717: Loss is 0.749, Accuracy is 54.7%\n",
            "Validation accuracy: 54.2%\n",
            "Batch #286/717: Loss is 0.757, Accuracy is 54.2%\n",
            "Batch #287/717: Loss is 0.741, Accuracy is 60.4%\n",
            "Batch #288/717: Loss is 0.766, Accuracy is 47.4%\n",
            "Batch #289/717: Loss is 0.752, Accuracy is 52.6%\n",
            "Batch #290/717: Loss is 0.754, Accuracy is 55.7%\n",
            "Batch #291/717: Loss is 0.760, Accuracy is 49.0%\n",
            "Batch #292/717: Loss is 0.755, Accuracy is 52.1%\n",
            "Batch #293/717: Loss is 0.744, Accuracy is 55.7%\n",
            "Batch #294/717: Loss is 0.753, Accuracy is 53.6%\n",
            "Batch #295/717: Loss is 0.736, Accuracy is 56.2%\n",
            "Batch #296/717: Loss is 0.747, Accuracy is 55.7%\n",
            "Batch #297/717: Loss is 0.732, Accuracy is 58.3%\n",
            "Batch #298/717: Loss is 0.741, Accuracy is 56.8%\n",
            "Batch #299/717: Loss is 0.745, Accuracy is 54.7%\n",
            "Batch #300/717: Loss is 0.749, Accuracy is 55.2%\n",
            "Validation accuracy: 52.1%\n",
            "Batch #301/717: Loss is 0.764, Accuracy is 49.0%\n",
            "Batch #302/717: Loss is 0.757, Accuracy is 50.0%\n",
            "Batch #303/717: Loss is 0.749, Accuracy is 51.0%\n",
            "Batch #304/717: Loss is 0.751, Accuracy is 53.1%\n",
            "Batch #305/717: Loss is 0.758, Accuracy is 49.0%\n",
            "Batch #306/717: Loss is 0.755, Accuracy is 53.6%\n",
            "Batch #307/717: Loss is 0.747, Accuracy is 53.6%\n",
            "Batch #308/717: Loss is 0.764, Accuracy is 46.4%\n",
            "Batch #309/717: Loss is 0.759, Accuracy is 55.7%\n",
            "Batch #310/717: Loss is 0.769, Accuracy is 50.5%\n",
            "Batch #311/717: Loss is 0.762, Accuracy is 49.5%\n",
            "Batch #312/717: Loss is 0.758, Accuracy is 51.6%\n",
            "Batch #313/717: Loss is 0.737, Accuracy is 57.3%\n",
            "Batch #314/717: Loss is 0.754, Accuracy is 52.1%\n",
            "Batch #315/717: Loss is 0.734, Accuracy is 60.9%\n",
            "Validation accuracy: 52.1%\n",
            "Batch #316/717: Loss is 0.738, Accuracy is 56.8%\n",
            "Batch #317/717: Loss is 0.771, Accuracy is 46.4%\n",
            "Batch #318/717: Loss is 0.767, Accuracy is 48.4%\n",
            "Batch #319/717: Loss is 0.758, Accuracy is 49.5%\n",
            "Batch #320/717: Loss is 0.757, Accuracy is 49.0%\n",
            "Batch #321/717: Loss is 0.736, Accuracy is 55.7%\n",
            "Batch #322/717: Loss is 0.755, Accuracy is 50.0%\n",
            "Batch #323/717: Loss is 0.755, Accuracy is 51.6%\n",
            "Batch #324/717: Loss is 0.753, Accuracy is 52.1%\n",
            "Batch #325/717: Loss is 0.757, Accuracy is 52.1%\n",
            "Batch #326/717: Loss is 0.766, Accuracy is 47.4%\n",
            "Batch #327/717: Loss is 0.768, Accuracy is 43.2%\n",
            "Batch #328/717: Loss is 0.763, Accuracy is 48.4%\n",
            "Batch #329/717: Loss is 0.756, Accuracy is 50.0%\n",
            "Batch #330/717: Loss is 0.753, Accuracy is 51.6%\n",
            "Validation accuracy: 46.9%\n",
            "Batch #331/717: Loss is 0.757, Accuracy is 49.0%\n",
            "Batch #332/717: Loss is 0.760, Accuracy is 47.9%\n",
            "Batch #333/717: Loss is 0.754, Accuracy is 50.5%\n",
            "Batch #334/717: Loss is 0.764, Accuracy is 48.4%\n",
            "Batch #335/717: Loss is 0.743, Accuracy is 56.2%\n",
            "Batch #336/717: Loss is 0.758, Accuracy is 44.3%\n",
            "Batch #337/717: Loss is 0.767, Accuracy is 42.7%\n",
            "Batch #338/717: Loss is 0.764, Accuracy is 48.4%\n",
            "Batch #339/717: Loss is 0.767, Accuracy is 51.0%\n",
            "Batch #340/717: Loss is 0.758, Accuracy is 49.0%\n",
            "Batch #341/717: Loss is 0.756, Accuracy is 50.5%\n",
            "Batch #342/717: Loss is 0.740, Accuracy is 56.8%\n",
            "Batch #343/717: Loss is 0.748, Accuracy is 51.6%\n",
            "Batch #344/717: Loss is 0.753, Accuracy is 50.5%\n",
            "Batch #345/717: Loss is 0.750, Accuracy is 54.2%\n",
            "Validation accuracy: 49.0%\n",
            "Batch #346/717: Loss is 0.755, Accuracy is 49.5%\n",
            "Batch #347/717: Loss is 0.752, Accuracy is 52.6%\n",
            "Batch #348/717: Loss is 0.765, Accuracy is 47.4%\n",
            "Batch #349/717: Loss is 0.752, Accuracy is 53.6%\n",
            "Batch #350/717: Loss is 0.752, Accuracy is 49.0%\n",
            "Batch #351/717: Loss is 0.765, Accuracy is 44.8%\n",
            "Batch #352/717: Loss is 0.753, Accuracy is 55.7%\n",
            "Batch #353/717: Loss is 0.750, Accuracy is 56.8%\n",
            "Batch #354/717: Loss is 0.754, Accuracy is 55.7%\n",
            "Batch #355/717: Loss is 0.757, Accuracy is 50.0%\n",
            "Batch #356/717: Loss is 0.779, Accuracy is 40.1%\n",
            "Batch #357/717: Loss is 0.748, Accuracy is 54.7%\n",
            "Batch #358/717: Loss is 0.763, Accuracy is 44.3%\n",
            "Batch #359/717: Loss is 0.747, Accuracy is 57.8%\n",
            "Batch #360/717: Loss is 0.757, Accuracy is 51.6%\n",
            "Validation accuracy: 49.0%\n",
            "Batch #361/717: Loss is 0.745, Accuracy is 55.2%\n",
            "Batch #362/717: Loss is 0.753, Accuracy is 53.1%\n",
            "Batch #363/717: Loss is 0.763, Accuracy is 46.9%\n",
            "Batch #364/717: Loss is 0.757, Accuracy is 47.4%\n",
            "Batch #365/717: Loss is 0.752, Accuracy is 55.2%\n",
            "Batch #366/717: Loss is 0.763, Accuracy is 50.5%\n",
            "Batch #367/717: Loss is 0.755, Accuracy is 50.0%\n",
            "Batch #368/717: Loss is 0.753, Accuracy is 52.1%\n",
            "Batch #369/717: Loss is 0.757, Accuracy is 48.4%\n",
            "Batch #370/717: Loss is 0.759, Accuracy is 50.0%\n",
            "Batch #371/717: Loss is 0.762, Accuracy is 49.5%\n",
            "Batch #372/717: Loss is 0.762, Accuracy is 49.0%\n",
            "Batch #373/717: Loss is 0.757, Accuracy is 54.2%\n",
            "Batch #374/717: Loss is 0.740, Accuracy is 53.6%\n",
            "Batch #375/717: Loss is 0.740, Accuracy is 56.8%\n",
            "Validation accuracy: 60.4%\n",
            "Batch #376/717: Loss is 0.754, Accuracy is 56.2%\n",
            "Batch #377/717: Loss is 0.743, Accuracy is 59.4%\n",
            "Batch #378/717: Loss is 0.743, Accuracy is 59.9%\n",
            "Batch #379/717: Loss is 0.744, Accuracy is 63.0%\n",
            "Batch #380/717: Loss is 0.768, Accuracy is 51.6%\n",
            "Batch #381/717: Loss is 0.756, Accuracy is 54.2%\n",
            "Batch #382/717: Loss is 0.758, Accuracy is 52.6%\n",
            "Batch #383/717: Loss is 0.752, Accuracy is 54.2%\n",
            "Batch #384/717: Loss is 0.766, Accuracy is 50.0%\n",
            "Batch #385/717: Loss is 0.751, Accuracy is 53.1%\n",
            "Batch #386/717: Loss is 0.766, Accuracy is 52.6%\n",
            "Batch #387/717: Loss is 0.763, Accuracy is 45.3%\n",
            "Batch #388/717: Loss is 0.772, Accuracy is 46.4%\n",
            "Batch #389/717: Loss is 0.760, Accuracy is 51.6%\n",
            "Batch #390/717: Loss is 0.747, Accuracy is 55.2%\n",
            "Validation accuracy: 53.1%\n",
            "Batch #391/717: Loss is 0.768, Accuracy is 49.0%\n",
            "Batch #392/717: Loss is 0.751, Accuracy is 51.0%\n",
            "Batch #393/717: Loss is 0.756, Accuracy is 47.4%\n",
            "Batch #394/717: Loss is 0.754, Accuracy is 46.4%\n",
            "Batch #395/717: Loss is 0.753, Accuracy is 47.9%\n",
            "Batch #396/717: Loss is 0.746, Accuracy is 56.2%\n",
            "Batch #397/717: Loss is 0.758, Accuracy is 49.0%\n",
            "Batch #398/717: Loss is 0.745, Accuracy is 51.0%\n",
            "Batch #399/717: Loss is 0.749, Accuracy is 52.6%\n",
            "Batch #400/717: Loss is 0.746, Accuracy is 52.6%\n",
            "Batch #401/717: Loss is 0.753, Accuracy is 51.6%\n",
            "Batch #402/717: Loss is 0.764, Accuracy is 46.4%\n",
            "Batch #403/717: Loss is 0.748, Accuracy is 55.2%\n",
            "Batch #404/717: Loss is 0.759, Accuracy is 50.0%\n",
            "Batch #405/717: Loss is 0.749, Accuracy is 49.0%\n",
            "Validation accuracy: 57.3%\n",
            "Batch #406/717: Loss is 0.762, Accuracy is 45.8%\n",
            "Batch #407/717: Loss is 0.756, Accuracy is 50.0%\n",
            "Batch #408/717: Loss is 0.755, Accuracy is 52.1%\n",
            "Batch #409/717: Loss is 0.758, Accuracy is 50.5%\n",
            "Batch #410/717: Loss is 0.742, Accuracy is 57.3%\n",
            "Batch #411/717: Loss is 0.775, Accuracy is 44.3%\n",
            "Batch #412/717: Loss is 0.753, Accuracy is 54.2%\n",
            "Batch #413/717: Loss is 0.757, Accuracy is 49.0%\n",
            "Batch #414/717: Loss is 0.742, Accuracy is 56.2%\n",
            "Batch #415/717: Loss is 0.755, Accuracy is 47.9%\n",
            "Batch #416/717: Loss is 0.746, Accuracy is 55.7%\n",
            "Batch #417/717: Loss is 0.752, Accuracy is 53.1%\n",
            "Batch #418/717: Loss is 0.752, Accuracy is 51.0%\n",
            "Batch #419/717: Loss is 0.767, Accuracy is 49.5%\n",
            "Batch #420/717: Loss is 0.758, Accuracy is 50.5%\n",
            "Validation accuracy: 51.0%\n",
            "Batch #421/717: Loss is 0.774, Accuracy is 46.4%\n",
            "Batch #422/717: Loss is 0.759, Accuracy is 47.4%\n",
            "Batch #423/717: Loss is 0.755, Accuracy is 53.6%\n",
            "Batch #424/717: Loss is 0.758, Accuracy is 50.5%\n",
            "Batch #425/717: Loss is 0.761, Accuracy is 47.4%\n",
            "Batch #426/717: Loss is 0.752, Accuracy is 51.6%\n",
            "Batch #427/717: Loss is 0.760, Accuracy is 46.9%\n",
            "Batch #428/717: Loss is 0.759, Accuracy is 48.4%\n",
            "Batch #429/717: Loss is 0.757, Accuracy is 47.4%\n",
            "Batch #430/717: Loss is 0.758, Accuracy is 46.9%\n",
            "Batch #431/717: Loss is 0.770, Accuracy is 44.8%\n",
            "Batch #432/717: Loss is 0.742, Accuracy is 56.2%\n",
            "Batch #433/717: Loss is 0.751, Accuracy is 53.6%\n",
            "Batch #434/717: Loss is 0.759, Accuracy is 46.4%\n",
            "Batch #435/717: Loss is 0.740, Accuracy is 54.2%\n",
            "Validation accuracy: 50.0%\n",
            "Batch #436/717: Loss is 0.763, Accuracy is 49.0%\n",
            "Batch #437/717: Loss is 0.744, Accuracy is 52.1%\n",
            "Batch #438/717: Loss is 0.753, Accuracy is 52.1%\n",
            "Batch #439/717: Loss is 0.761, Accuracy is 47.9%\n",
            "Batch #440/717: Loss is 0.753, Accuracy is 53.1%\n",
            "Batch #441/717: Loss is 0.744, Accuracy is 55.2%\n",
            "Batch #442/717: Loss is 0.752, Accuracy is 52.1%\n",
            "Batch #443/717: Loss is 0.768, Accuracy is 47.4%\n",
            "Batch #444/717: Loss is 0.755, Accuracy is 50.0%\n",
            "Batch #445/717: Loss is 0.759, Accuracy is 46.9%\n",
            "Batch #446/717: Loss is 0.743, Accuracy is 56.2%\n",
            "Batch #447/717: Loss is 0.747, Accuracy is 54.2%\n",
            "Batch #448/717: Loss is 0.762, Accuracy is 46.9%\n",
            "Batch #449/717: Loss is 0.746, Accuracy is 51.0%\n",
            "Batch #450/717: Loss is 0.758, Accuracy is 52.6%\n",
            "Validation accuracy: 54.7%\n",
            "Batch #451/717: Loss is 0.755, Accuracy is 49.0%\n",
            "Batch #452/717: Loss is 0.755, Accuracy is 49.5%\n",
            "Batch #453/717: Loss is 0.743, Accuracy is 56.2%\n",
            "Batch #454/717: Loss is 0.777, Accuracy is 42.7%\n",
            "Batch #455/717: Loss is 0.751, Accuracy is 53.1%\n",
            "Batch #456/717: Loss is 0.753, Accuracy is 52.1%\n",
            "Batch #457/717: Loss is 0.754, Accuracy is 54.7%\n",
            "Batch #458/717: Loss is 0.758, Accuracy is 50.0%\n",
            "Batch #459/717: Loss is 0.756, Accuracy is 51.0%\n",
            "Batch #460/717: Loss is 0.771, Accuracy is 45.3%\n",
            "Batch #461/717: Loss is 0.742, Accuracy is 56.8%\n",
            "Batch #462/717: Loss is 0.744, Accuracy is 55.2%\n",
            "Batch #463/717: Loss is 0.757, Accuracy is 50.0%\n",
            "Batch #464/717: Loss is 0.747, Accuracy is 54.7%\n",
            "Batch #465/717: Loss is 0.762, Accuracy is 50.5%\n",
            "Validation accuracy: 56.2%\n",
            "Batch #466/717: Loss is 0.752, Accuracy is 52.1%\n",
            "Batch #467/717: Loss is 0.747, Accuracy is 55.2%\n",
            "Batch #468/717: Loss is 0.746, Accuracy is 55.7%\n",
            "Batch #469/717: Loss is 0.766, Accuracy is 47.9%\n",
            "Batch #470/717: Loss is 0.745, Accuracy is 53.6%\n",
            "Batch #471/717: Loss is 0.747, Accuracy is 53.1%\n",
            "Batch #472/717: Loss is 0.764, Accuracy is 49.0%\n",
            "Batch #473/717: Loss is 0.759, Accuracy is 49.0%\n",
            "Batch #474/717: Loss is 0.760, Accuracy is 49.0%\n",
            "Batch #475/717: Loss is 0.770, Accuracy is 43.8%\n",
            "Batch #476/717: Loss is 0.763, Accuracy is 43.8%\n",
            "Batch #477/717: Loss is 0.739, Accuracy is 56.2%\n",
            "Batch #478/717: Loss is 0.766, Accuracy is 46.4%\n",
            "Batch #479/717: Loss is 0.746, Accuracy is 51.0%\n",
            "Batch #480/717: Loss is 0.768, Accuracy is 42.2%\n",
            "Validation accuracy: 45.3%\n",
            "Batch #481/717: Loss is 0.747, Accuracy is 52.6%\n",
            "Batch #482/717: Loss is 0.760, Accuracy is 46.4%\n",
            "Batch #483/717: Loss is 0.745, Accuracy is 54.2%\n",
            "Batch #484/717: Loss is 0.755, Accuracy is 49.5%\n",
            "Batch #485/717: Loss is 0.762, Accuracy is 52.1%\n",
            "Batch #486/717: Loss is 0.751, Accuracy is 52.6%\n",
            "Batch #487/717: Loss is 0.746, Accuracy is 56.2%\n",
            "Batch #488/717: Loss is 0.744, Accuracy is 58.3%\n",
            "Batch #489/717: Loss is 0.751, Accuracy is 52.1%\n",
            "Batch #490/717: Loss is 0.743, Accuracy is 62.5%\n",
            "Batch #491/717: Loss is 0.763, Accuracy is 51.6%\n",
            "Batch #492/717: Loss is 0.757, Accuracy is 56.8%\n",
            "Batch #493/717: Loss is 0.747, Accuracy is 57.8%\n",
            "Batch #494/717: Loss is 0.762, Accuracy is 51.0%\n",
            "Batch #495/717: Loss is 0.761, Accuracy is 55.2%\n",
            "Validation accuracy: 58.9%\n",
            "Batch #496/717: Loss is 0.774, Accuracy is 51.0%\n",
            "Batch #497/717: Loss is 0.763, Accuracy is 55.2%\n",
            "Batch #498/717: Loss is 0.747, Accuracy is 58.3%\n",
            "Batch #499/717: Loss is 0.762, Accuracy is 49.5%\n",
            "Batch #500/717: Loss is 0.763, Accuracy is 52.6%\n",
            "Batch #501/717: Loss is 0.759, Accuracy is 51.6%\n",
            "Batch #502/717: Loss is 0.730, Accuracy is 64.1%\n",
            "Batch #503/717: Loss is 0.762, Accuracy is 48.4%\n",
            "Batch #504/717: Loss is 0.765, Accuracy is 48.4%\n",
            "Batch #505/717: Loss is 0.740, Accuracy is 58.9%\n",
            "Batch #506/717: Loss is 0.759, Accuracy is 47.9%\n",
            "Batch #507/717: Loss is 0.758, Accuracy is 50.5%\n",
            "Batch #508/717: Loss is 0.760, Accuracy is 45.8%\n",
            "Batch #509/717: Loss is 0.765, Accuracy is 47.9%\n",
            "Batch #510/717: Loss is 0.764, Accuracy is 45.3%\n",
            "Validation accuracy: 56.2%\n",
            "Batch #511/717: Loss is 0.765, Accuracy is 45.8%\n",
            "Batch #512/717: Loss is 0.759, Accuracy is 48.4%\n",
            "Batch #513/717: Loss is 0.750, Accuracy is 49.0%\n",
            "Batch #514/717: Loss is 0.752, Accuracy is 54.2%\n",
            "Batch #515/717: Loss is 0.757, Accuracy is 50.5%\n",
            "Batch #516/717: Loss is 0.759, Accuracy is 49.0%\n",
            "Batch #517/717: Loss is 0.759, Accuracy is 46.9%\n",
            "Batch #518/717: Loss is 0.757, Accuracy is 49.5%\n",
            "Batch #519/717: Loss is 0.758, Accuracy is 50.0%\n",
            "Batch #520/717: Loss is 0.757, Accuracy is 49.5%\n",
            "Batch #521/717: Loss is 0.742, Accuracy is 55.2%\n",
            "Batch #522/717: Loss is 0.748, Accuracy is 51.0%\n",
            "Batch #523/717: Loss is 0.764, Accuracy is 46.4%\n",
            "Batch #524/717: Loss is 0.759, Accuracy is 49.0%\n",
            "Batch #525/717: Loss is 0.775, Accuracy is 40.1%\n",
            "Validation accuracy: 52.1%\n",
            "Batch #526/717: Loss is 0.750, Accuracy is 51.6%\n",
            "Batch #527/717: Loss is 0.748, Accuracy is 54.7%\n",
            "Batch #528/717: Loss is 0.751, Accuracy is 52.1%\n",
            "Batch #529/717: Loss is 0.735, Accuracy is 62.0%\n",
            "Batch #530/717: Loss is 0.750, Accuracy is 52.6%\n",
            "Batch #531/717: Loss is 0.744, Accuracy is 53.1%\n",
            "Batch #532/717: Loss is 0.751, Accuracy is 48.4%\n",
            "Batch #533/717: Loss is 0.736, Accuracy is 56.2%\n",
            "Batch #534/717: Loss is 0.747, Accuracy is 55.7%\n",
            "Batch #535/717: Loss is 0.762, Accuracy is 44.8%\n",
            "Batch #536/717: Loss is 0.768, Accuracy is 51.0%\n",
            "Batch #537/717: Loss is 0.757, Accuracy is 47.9%\n",
            "Batch #538/717: Loss is 0.757, Accuracy is 47.9%\n",
            "Batch #539/717: Loss is 0.774, Accuracy is 43.8%\n",
            "Batch #540/717: Loss is 0.753, Accuracy is 53.1%\n",
            "Validation accuracy: 53.1%\n",
            "Batch #541/717: Loss is 0.744, Accuracy is 56.8%\n",
            "Batch #542/717: Loss is 0.758, Accuracy is 51.6%\n",
            "Batch #543/717: Loss is 0.751, Accuracy is 54.2%\n",
            "Batch #544/717: Loss is 0.756, Accuracy is 50.5%\n",
            "Batch #545/717: Loss is 0.765, Accuracy is 50.5%\n",
            "Batch #546/717: Loss is 0.741, Accuracy is 54.7%\n",
            "Batch #547/717: Loss is 0.743, Accuracy is 54.2%\n",
            "Batch #548/717: Loss is 0.754, Accuracy is 55.2%\n",
            "Batch #549/717: Loss is 0.766, Accuracy is 50.0%\n",
            "Batch #550/717: Loss is 0.760, Accuracy is 49.0%\n",
            "Batch #551/717: Loss is 0.772, Accuracy is 50.0%\n",
            "Batch #552/717: Loss is 0.756, Accuracy is 52.6%\n",
            "Batch #553/717: Loss is 0.748, Accuracy is 54.2%\n",
            "Batch #554/717: Loss is 0.748, Accuracy is 55.2%\n",
            "Batch #555/717: Loss is 0.760, Accuracy is 50.5%\n",
            "Validation accuracy: 50.0%\n",
            "Batch #556/717: Loss is 0.757, Accuracy is 50.0%\n",
            "Batch #557/717: Loss is 0.741, Accuracy is 61.5%\n",
            "Batch #558/717: Loss is 0.754, Accuracy is 52.1%\n",
            "Batch #559/717: Loss is 0.754, Accuracy is 52.6%\n",
            "Batch #560/717: Loss is 0.730, Accuracy is 60.4%\n",
            "Batch #561/717: Loss is 0.751, Accuracy is 51.6%\n",
            "Batch #562/717: Loss is 0.729, Accuracy is 59.4%\n",
            "Batch #563/717: Loss is 0.764, Accuracy is 47.4%\n",
            "Batch #564/717: Loss is 0.744, Accuracy is 55.2%\n",
            "Batch #565/717: Loss is 0.740, Accuracy is 54.7%\n",
            "Batch #566/717: Loss is 0.724, Accuracy is 63.5%\n",
            "Batch #567/717: Loss is 0.751, Accuracy is 57.8%\n",
            "Batch #568/717: Loss is 0.743, Accuracy is 64.6%\n",
            "Batch #569/717: Loss is 0.751, Accuracy is 57.8%\n",
            "Batch #570/717: Loss is 0.756, Accuracy is 57.3%\n",
            "Validation accuracy: 63.5%\n",
            "Batch #571/717: Loss is 0.763, Accuracy is 56.8%\n",
            "Batch #572/717: Loss is 0.726, Accuracy is 69.3%\n",
            "Batch #573/717: Loss is 0.761, Accuracy is 58.3%\n",
            "Batch #574/717: Loss is 0.772, Accuracy is 55.2%\n",
            "Batch #575/717: Loss is 0.754, Accuracy is 57.3%\n",
            "Batch #576/717: Loss is 0.767, Accuracy is 55.7%\n",
            "Batch #577/717: Loss is 0.757, Accuracy is 57.8%\n",
            "Batch #578/717: Loss is 0.759, Accuracy is 60.9%\n",
            "Batch #579/717: Loss is 0.763, Accuracy is 58.9%\n",
            "Batch #580/717: Loss is 0.745, Accuracy is 60.9%\n",
            "Batch #581/717: Loss is 0.759, Accuracy is 56.2%\n",
            "Batch #582/717: Loss is 0.758, Accuracy is 53.6%\n",
            "Batch #583/717: Loss is 0.745, Accuracy is 59.9%\n",
            "Batch #584/717: Loss is 0.753, Accuracy is 54.2%\n",
            "Batch #585/717: Loss is 0.736, Accuracy is 61.5%\n",
            "Validation accuracy: 53.1%\n",
            "Batch #586/717: Loss is 0.744, Accuracy is 58.3%\n",
            "Batch #587/717: Loss is 0.755, Accuracy is 53.1%\n",
            "Batch #588/717: Loss is 0.751, Accuracy is 54.7%\n",
            "Batch #589/717: Loss is 0.758, Accuracy is 52.1%\n",
            "Batch #590/717: Loss is 0.754, Accuracy is 53.6%\n",
            "Batch #591/717: Loss is 0.758, Accuracy is 48.4%\n",
            "Batch #592/717: Loss is 0.755, Accuracy is 51.0%\n",
            "Batch #593/717: Loss is 0.744, Accuracy is 53.6%\n",
            "Batch #594/717: Loss is 0.754, Accuracy is 54.7%\n",
            "Batch #595/717: Loss is 0.754, Accuracy is 54.2%\n",
            "Batch #596/717: Loss is 0.740, Accuracy is 56.2%\n",
            "Batch #597/717: Loss is 0.754, Accuracy is 49.5%\n",
            "Batch #598/717: Loss is 0.761, Accuracy is 45.8%\n",
            "Batch #599/717: Loss is 0.761, Accuracy is 50.5%\n",
            "Batch #600/717: Loss is 0.749, Accuracy is 51.0%\n",
            "Validation accuracy: 48.4%\n",
            "Batch #601/717: Loss is 0.748, Accuracy is 54.2%\n",
            "Batch #602/717: Loss is 0.756, Accuracy is 51.0%\n",
            "Batch #603/717: Loss is 0.744, Accuracy is 53.1%\n",
            "Batch #604/717: Loss is 0.747, Accuracy is 54.2%\n",
            "Batch #605/717: Loss is 0.763, Accuracy is 47.4%\n",
            "Batch #606/717: Loss is 0.759, Accuracy is 49.5%\n",
            "Batch #607/717: Loss is 0.756, Accuracy is 49.5%\n",
            "Batch #608/717: Loss is 0.757, Accuracy is 52.1%\n",
            "Batch #609/717: Loss is 0.753, Accuracy is 53.6%\n",
            "Batch #610/717: Loss is 0.749, Accuracy is 53.6%\n",
            "Batch #611/717: Loss is 0.757, Accuracy is 46.4%\n",
            "Batch #612/717: Loss is 0.763, Accuracy is 47.4%\n",
            "Batch #613/717: Loss is 0.751, Accuracy is 53.1%\n",
            "Batch #614/717: Loss is 0.752, Accuracy is 51.6%\n",
            "Batch #615/717: Loss is 0.738, Accuracy is 58.3%\n",
            "Validation accuracy: 46.9%\n",
            "Batch #616/717: Loss is 0.758, Accuracy is 51.6%\n",
            "Batch #617/717: Loss is 0.759, Accuracy is 49.0%\n",
            "Batch #618/717: Loss is 0.737, Accuracy is 55.7%\n",
            "Batch #619/717: Loss is 0.758, Accuracy is 47.9%\n",
            "Batch #620/717: Loss is 0.759, Accuracy is 47.9%\n",
            "Batch #621/717: Loss is 0.759, Accuracy is 49.5%\n",
            "Batch #622/717: Loss is 0.756, Accuracy is 49.5%\n",
            "Batch #623/717: Loss is 0.761, Accuracy is 51.6%\n",
            "Batch #624/717: Loss is 0.746, Accuracy is 53.6%\n",
            "Batch #625/717: Loss is 0.765, Accuracy is 47.4%\n",
            "Batch #626/717: Loss is 0.777, Accuracy is 40.6%\n",
            "Batch #627/717: Loss is 0.759, Accuracy is 47.9%\n",
            "Batch #628/717: Loss is 0.753, Accuracy is 51.0%\n",
            "Batch #629/717: Loss is 0.765, Accuracy is 45.3%\n",
            "Batch #630/717: Loss is 0.759, Accuracy is 47.4%\n",
            "Validation accuracy: 54.2%\n",
            "Batch #631/717: Loss is 0.741, Accuracy is 56.2%\n",
            "Batch #632/717: Loss is 0.756, Accuracy is 47.4%\n",
            "Batch #633/717: Loss is 0.758, Accuracy is 51.0%\n",
            "Batch #634/717: Loss is 0.742, Accuracy is 53.6%\n",
            "Batch #635/717: Loss is 0.743, Accuracy is 52.1%\n",
            "Batch #636/717: Loss is 0.734, Accuracy is 58.9%\n",
            "Batch #637/717: Loss is 0.760, Accuracy is 51.0%\n",
            "Batch #638/717: Loss is 0.769, Accuracy is 45.3%\n",
            "Batch #639/717: Loss is 0.768, Accuracy is 47.4%\n",
            "Batch #640/717: Loss is 0.766, Accuracy is 49.0%\n",
            "Batch #641/717: Loss is 0.749, Accuracy is 56.8%\n",
            "Batch #642/717: Loss is 0.743, Accuracy is 55.7%\n",
            "Batch #643/717: Loss is 0.756, Accuracy is 53.1%\n",
            "Batch #644/717: Loss is 0.757, Accuracy is 51.0%\n",
            "Batch #645/717: Loss is 0.751, Accuracy is 54.2%\n",
            "Validation accuracy: 51.6%\n",
            "Batch #646/717: Loss is 0.761, Accuracy is 50.5%\n",
            "Batch #647/717: Loss is 0.753, Accuracy is 52.6%\n",
            "Batch #648/717: Loss is 0.764, Accuracy is 47.9%\n",
            "Batch #649/717: Loss is 0.758, Accuracy is 52.1%\n",
            "Batch #650/717: Loss is 0.755, Accuracy is 55.2%\n",
            "Batch #651/717: Loss is 0.761, Accuracy is 51.0%\n",
            "Batch #652/717: Loss is 0.745, Accuracy is 60.4%\n",
            "Batch #653/717: Loss is 0.760, Accuracy is 50.5%\n",
            "Batch #654/717: Loss is 0.747, Accuracy is 56.8%\n",
            "Batch #655/717: Loss is 0.748, Accuracy is 56.8%\n",
            "Batch #656/717: Loss is 0.761, Accuracy is 47.4%\n",
            "Batch #657/717: Loss is 0.741, Accuracy is 56.2%\n",
            "Batch #658/717: Loss is 0.759, Accuracy is 53.1%\n",
            "Batch #659/717: Loss is 0.755, Accuracy is 52.1%\n",
            "Batch #660/717: Loss is 0.750, Accuracy is 54.2%\n",
            "Validation accuracy: 52.1%\n",
            "Batch #661/717: Loss is 0.756, Accuracy is 51.0%\n",
            "Batch #662/717: Loss is 0.750, Accuracy is 55.7%\n",
            "Batch #663/717: Loss is 0.750, Accuracy is 51.6%\n",
            "Batch #664/717: Loss is 0.749, Accuracy is 53.6%\n",
            "Batch #665/717: Loss is 0.734, Accuracy is 56.2%\n",
            "Batch #666/717: Loss is 0.754, Accuracy is 49.5%\n",
            "Batch #667/717: Loss is 0.758, Accuracy is 54.7%\n",
            "Batch #668/717: Loss is 0.765, Accuracy is 46.4%\n",
            "Batch #669/717: Loss is 0.756, Accuracy is 51.6%\n",
            "Batch #670/717: Loss is 0.761, Accuracy is 48.4%\n",
            "Batch #671/717: Loss is 0.751, Accuracy is 50.5%\n",
            "Batch #672/717: Loss is 0.768, Accuracy is 45.3%\n",
            "Batch #673/717: Loss is 0.760, Accuracy is 52.1%\n",
            "Batch #674/717: Loss is 0.736, Accuracy is 57.3%\n",
            "Batch #675/717: Loss is 0.744, Accuracy is 57.3%\n",
            "Validation accuracy: 53.6%\n",
            "Batch #676/717: Loss is 0.748, Accuracy is 57.8%\n",
            "Batch #677/717: Loss is 0.769, Accuracy is 48.4%\n",
            "Batch #678/717: Loss is 0.752, Accuracy is 57.8%\n",
            "Batch #679/717: Loss is 0.763, Accuracy is 54.2%\n",
            "Batch #680/717: Loss is 0.751, Accuracy is 57.8%\n",
            "Batch #681/717: Loss is 0.747, Accuracy is 55.2%\n",
            "Batch #682/717: Loss is 0.764, Accuracy is 53.1%\n",
            "Batch #683/717: Loss is 0.762, Accuracy is 50.0%\n",
            "Batch #684/717: Loss is 0.741, Accuracy is 55.7%\n",
            "Batch #685/717: Loss is 0.743, Accuracy is 60.9%\n",
            "Batch #686/717: Loss is 0.744, Accuracy is 57.3%\n",
            "Batch #687/717: Loss is 0.744, Accuracy is 53.6%\n",
            "Batch #688/717: Loss is 0.756, Accuracy is 53.1%\n",
            "Batch #689/717: Loss is 0.742, Accuracy is 53.6%\n",
            "Batch #690/717: Loss is 0.744, Accuracy is 55.2%\n",
            "Validation accuracy: 51.6%\n",
            "Batch #691/717: Loss is 0.738, Accuracy is 55.7%\n",
            "Batch #692/717: Loss is 0.738, Accuracy is 57.8%\n",
            "Batch #693/717: Loss is 0.738, Accuracy is 62.0%\n",
            "Batch #694/717: Loss is 0.772, Accuracy is 47.4%\n",
            "Batch #695/717: Loss is 0.757, Accuracy is 48.4%\n",
            "Batch #696/717: Loss is 0.757, Accuracy is 50.5%\n",
            "Batch #697/717: Loss is 0.747, Accuracy is 55.2%\n",
            "Batch #698/717: Loss is 0.765, Accuracy is 51.6%\n",
            "Batch #699/717: Loss is 0.755, Accuracy is 56.8%\n",
            "Batch #700/717: Loss is 0.760, Accuracy is 51.6%\n",
            "Batch #701/717: Loss is 0.760, Accuracy is 55.7%\n",
            "Batch #702/717: Loss is 0.754, Accuracy is 52.1%\n",
            "Batch #703/717: Loss is 0.760, Accuracy is 46.4%\n",
            "Batch #704/717: Loss is 0.761, Accuracy is 51.0%\n",
            "Batch #705/717: Loss is 0.752, Accuracy is 52.1%\n",
            "Validation accuracy: 50.0%\n",
            "Batch #706/717: Loss is 0.767, Accuracy is 45.8%\n",
            "Batch #707/717: Loss is 0.774, Accuracy is 42.7%\n",
            "Batch #708/717: Loss is 0.765, Accuracy is 47.9%\n",
            "Batch #709/717: Loss is 0.759, Accuracy is 50.5%\n",
            "Batch #710/717: Loss is 0.761, Accuracy is 46.4%\n",
            "Batch #711/717: Loss is 0.764, Accuracy is 45.8%\n",
            "Batch #712/717: Loss is 0.746, Accuracy is 54.7%\n",
            "Batch #713/717: Loss is 0.749, Accuracy is 52.1%\n",
            "Batch #714/717: Loss is 0.751, Accuracy is 49.0%\n",
            "Batch #715/717: Loss is 0.764, Accuracy is 46.4%\n",
            "Batch #716/717: Loss is 0.760, Accuracy is 46.7%\n",
            "Epoch 2 of 5\n",
            "Batch #0/717: Loss is 0.768, Accuracy is 45.8%\n",
            "Validation accuracy: 49.0%\n",
            "Batch #1/717: Loss is 0.748, Accuracy is 50.5%\n",
            "Batch #2/717: Loss is 0.754, Accuracy is 50.5%\n",
            "Batch #3/717: Loss is 0.754, Accuracy is 52.1%\n",
            "Batch #4/717: Loss is 0.751, Accuracy is 53.1%\n",
            "Batch #5/717: Loss is 0.756, Accuracy is 50.5%\n",
            "Batch #6/717: Loss is 0.768, Accuracy is 44.8%\n",
            "Batch #7/717: Loss is 0.760, Accuracy is 45.3%\n",
            "Batch #8/717: Loss is 0.752, Accuracy is 52.1%\n",
            "Batch #9/717: Loss is 0.759, Accuracy is 51.6%\n",
            "Batch #10/717: Loss is 0.756, Accuracy is 50.0%\n",
            "Batch #11/717: Loss is 0.761, Accuracy is 45.3%\n",
            "Batch #12/717: Loss is 0.760, Accuracy is 51.6%\n",
            "Batch #13/717: Loss is 0.744, Accuracy is 59.4%\n",
            "Batch #14/717: Loss is 0.761, Accuracy is 47.4%\n",
            "Batch #15/717: Loss is 0.756, Accuracy is 47.4%\n",
            "Validation accuracy: 49.5%\n",
            "Batch #16/717: Loss is 0.761, Accuracy is 46.9%\n",
            "Batch #17/717: Loss is 0.762, Accuracy is 43.2%\n",
            "Batch #18/717: Loss is 0.766, Accuracy is 45.8%\n",
            "Batch #19/717: Loss is 0.741, Accuracy is 55.2%\n",
            "Batch #20/717: Loss is 0.754, Accuracy is 52.1%\n",
            "Batch #21/717: Loss is 0.739, Accuracy is 55.7%\n",
            "Batch #22/717: Loss is 0.757, Accuracy is 52.6%\n",
            "Batch #23/717: Loss is 0.744, Accuracy is 59.4%\n",
            "Batch #24/717: Loss is 0.753, Accuracy is 56.8%\n",
            "Batch #25/717: Loss is 0.751, Accuracy is 54.7%\n",
            "Batch #26/717: Loss is 0.738, Accuracy is 57.8%\n",
            "Batch #27/717: Loss is 0.756, Accuracy is 47.4%\n",
            "Batch #28/717: Loss is 0.751, Accuracy is 52.6%\n",
            "Batch #29/717: Loss is 0.742, Accuracy is 54.2%\n",
            "Batch #30/717: Loss is 0.759, Accuracy is 50.5%\n",
            "Validation accuracy: 52.6%\n",
            "Batch #31/717: Loss is 0.754, Accuracy is 49.0%\n",
            "Batch #32/717: Loss is 0.738, Accuracy is 57.3%\n",
            "Batch #33/717: Loss is 0.762, Accuracy is 46.9%\n",
            "Batch #34/717: Loss is 0.747, Accuracy is 51.0%\n",
            "Batch #35/717: Loss is 0.757, Accuracy is 50.5%\n",
            "Batch #36/717: Loss is 0.752, Accuracy is 54.2%\n",
            "Batch #37/717: Loss is 0.747, Accuracy is 54.2%\n",
            "Batch #38/717: Loss is 0.758, Accuracy is 47.9%\n",
            "Batch #39/717: Loss is 0.751, Accuracy is 50.5%\n",
            "Batch #40/717: Loss is 0.744, Accuracy is 56.8%\n",
            "Batch #41/717: Loss is 0.751, Accuracy is 54.2%\n",
            "Batch #42/717: Loss is 0.752, Accuracy is 50.0%\n",
            "Batch #43/717: Loss is 0.749, Accuracy is 54.7%\n",
            "Batch #44/717: Loss is 0.750, Accuracy is 51.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5912cfe9b395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1} of {epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-5912cfe9b395>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(iter_cap)\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0miter_without_improvement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtarget_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_types'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US0s44g1yMt1"
      },
      "source": [
        "## saving logic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q04CcJcdGGB8"
      },
      "source": [
        "import secrets\n",
        "identifier_string = secrets.token_hex(nbytes=1)\n",
        "\n",
        "category_model_name = f'category_model_{identifier_string}_state.pth'\n",
        "category_encoder_name = f'category_encoder_{identifier_string}_state.pth'\n",
        "\n",
        "\n",
        "torch.save(category_model.state_dict(), f\"/content/drive/MyDrive/Kool/Ülikool/3. aasta/Lõputöö/{category_model_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}